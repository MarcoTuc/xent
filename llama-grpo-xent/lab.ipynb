{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 02-18 18:53:01 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "from unsloth import is_bfloat16_supported\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: Tesla V100-DGXS-32GB. Max memory: 31.715 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit with actual GPU utilization = 59.24%\n",
      "Unsloth: Your GPU has CUDA compute capability 7.0 with VRAM = 31.72 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 256.\n",
      "Unsloth: vLLM's KV Cache can use up to 12.69 GB. Also swap space = 6 GB.\n",
      "WARNING 02-18 18:53:10 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 02-18 18:53:17 config.py:542] This model supports multiple tasks: {'generate', 'classify', 'score', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 02-18 18:53:17 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-18 18:53:19 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 02-18 18:53:19 cuda.py:227] Using XFormers backend.\n",
      "INFO 02-18 18:53:19 model_runner.py:1110] Starting to load model unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W218 18:53:19.580560178 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-18 18:53:19 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 02-18 18:53:20 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f18acad79b48d08dbe2767eb80d1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f86ea2c8377486bb5a090952390165a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-18 18:53:23 model_runner.py:1115] Loading model weights took 5.5976 GB\n",
      "INFO 02-18 18:53:23 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 02-18 18:53:27 worker.py:267] Memory profiling takes 3.60 seconds\n",
      "INFO 02-18 18:53:27 worker.py:267] the current vLLM instance can use total_gpu_memory (31.72GiB) x gpu_memory_utilization (0.59) = 18.79GiB\n",
      "INFO 02-18 18:53:27 worker.py:267] model weights take 5.60GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 11.89GiB.\n",
      "INFO 02-18 18:53:27 executor_base.py:110] # CUDA blocks: 6088, # CPU blocks: 3072\n",
      "INFO 02-18 18:53:27 executor_base.py:115] Maximum concurrency for 4096 tokens per request: 23.78x\n",
      "INFO 02-18 18:53:31 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:32<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-18 18:54:03 model_runner.py:1562] Graph capturing finished in 33 secs, took 4.57 GiB\n",
      "INFO 02-18 18:54:03 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 40.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.2.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 4096 # Can increase for longer reasoning traces\n",
    "lora_rank = 16 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.6, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "\n",
    "You estimate the integer-rounded cross-entropy of the text before a special function. \n",
    "\n",
    "Given a certain text before the function like:\n",
    "\n",
    "token_1token_2token_3token_4\n",
    "\n",
    "You are going to format it like:\n",
    "\n",
    "token_1: cross_entropy of token_1\n",
    "token_2: cross_entropy of token_2\n",
    "token_3: cross_entropy of token_3\n",
    "token_4: cross_entropy of token_4\n",
    "\n",
    "Tokens have to follow the same pattern of your own tokenizer.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f0cc6e64c14dacade837a005b65ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/161100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c323b2d35f44b9877dbec5ea574295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_response(data: str) -> list[dict]:\n",
    "    \"\"\" This function formats a response from xent format to a dictionary of tokens and xents as ints \"\"\"\n",
    "    linecond = r\"(?<=\\d)\\n\"\n",
    "    txcond = r\":(?=\\s\\d)\"\n",
    "    r_split = re.split(linecond, data[1:-1])\n",
    "    tx_split = [re.split(txcond, line) for line in r_split]\n",
    "    return [\n",
    "        {\n",
    "            \"token\": x[0],\n",
    "            \"xent\": int(x[1].strip())\n",
    "        }   for x in tx_split\n",
    "    ]\n",
    "\n",
    "def get_xent_dataset(test_size = 0.1) -> Dataset:\n",
    "    data_path = \"/rcp/marco/data/instruct_llama/closure_database.json\"\n",
    "    data = load_dataset(\"/rcp/marco/data/instruct_llama\", data_files=[data_path], split=\"train\")\n",
    "    data = data.train_test_split(test_size=test_size, shuffle=True)\n",
    "\n",
    "    dataset = data.map(\n",
    "        lambda x: {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": x[\"prompt\"]}\n",
    "            ],\n",
    "            \"answer\": x[\"response\"]\n",
    "        },\n",
    "        remove_columns=[\"response\"]\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = get_xent_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_reward(completions, **kwargs):\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    print(f\"{len(responses)} completions have been produced\")\n",
    "    for response in responses: \n",
    "        print(response)\n",
    "        print(\"\\n\\n\")\n",
    "    return 0\n",
    "\n",
    "def formatting_reward(completions, answer, **kwargs):\n",
    "    \"\"\" This reward function makes sure the output has a certain format \"\"\"\n",
    "    wf = 1\n",
    "    regex_line = r\"[\\S\\s]*?:\\s\\d{1,2}\\n\"\n",
    "    # regex_all = r\"({regex_line})+\" # if you wanna make a whole-thing reward instead of line per line\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    correct = [len(re.findall(regex_line, response)) for response in responses]\n",
    "    print(\"-\"*40)\n",
    "    for i, corr in enumerate(correct):\n",
    "        print(f\"Correctly formatted lines on response {i}: {corr}\")\n",
    "        if corr > 0:\n",
    "            print(responses[i])\n",
    "    return [c*wf if c > 0 else -5 for c in correct]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 3e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size = 6,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    num_generations = 6, # Decrease if out of memory\n",
    "    max_prompt_length = 2048,\n",
    "    max_completion_length = 2048,\n",
    "    num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    # max_steps = 250,\n",
    "    # save_steps = 250,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"trained_models/llama\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 161,100 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 6 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 6 | Total steps = 161,100\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 completions have been produced\n",
      "I can process the text you provided, but I want to note that the last line contains non-alphanumeric characters `@##$$##@ fwd_closure((integer))>:Ã§%Ã§>:`, which are not part of the usual tokenizer pattern. I will exclude those from the output. \n",
      "\n",
      "Here's the processed text:\n",
      "\n",
      "```python\n",
      "import re\n",
      "import math\n",
      "\n",
      "text = \"\"\"\n",
      "entinas\n",
      "AerolÃ­neas Argentinas destinations\n",
      "Afro Argentine\n",
      "Agriculture in Argentina\n",
      "AgrupaciÃ³n AÃ©rea Presidencial\n",
      "AgustÃ­n Calleri\n",
      "AgustÃ­n P. Justo\n",
      "AgustÃ­n Pichot\n",
      "AgustÃ­n Tosco\n",
      "Agustina Cherri\n",
      "Aero VIP\n",
      "Aimogasta\n",
      "Alberto Argibay\n",
      "Alberto Castillo\n",
      "Alberto CalderÃ³n\n",
      "Alberto Cortez\n",
      "Alberto de Mendoza\n",
      "Alberto FernÃ¡ndez de Rosa\n",
      "Alberto Gerchunoff\n",
      "Alberto Ginastera\n",
      "Alberto Olmedo\n",
      "Alberto Prebisch\n",
      "Alberto Tarantini\n",
      "Alberto Vaccarezza\n",
      "Aldo Duscher\n",
      "Alejandra Boero\n",
      "Alejandra Pizarnik\n",
      "Alejandro AgustÃ­n Lanusse\n",
      "Alejandro Awada\n",
      "Alejandro Bustillo\n",
      "Alejandro Doria\n",
      "Alejandro Dolina\n",
      "Alejandro GÃ³mez\n",
      "Alejandro Lerner\n",
      "Alejandro Mancuso\n",
      "Alejandro Romay\n",
      "Alejandro Sokol\n",
      "Alejo Carmen GuzmÃ¡n\n",
      "Alejo Castex\n",
      "Alejo Peyret\n",
      "Alfajor\n",
      "Alfonsina Storni\n",
      "@##$$##@ fwd_closure((integer))>:Ã§%Ã§>\n",
      "\"\"\"\n",
      "\n",
      "# Find all words\n",
      "words = re.findall(r'\\w+', text)\n",
      "\n",
      "# Define a dictionary to store the frequency of each word\n",
      "word_freq = {}\n",
      "for word in words:\n",
      "    if word not in word_freq:\n",
      "        word_freq[word] = 1\n",
      "    else:\n",
      "        word_freq[word] += 1\n",
      "\n",
      "# Calculate the cross entropy for each word\n",
      "for word in word_freq.keys():\n",
      "    p = word_freq[word] / len(words)\n",
      "    entropy = -p * math.log2(p)\n",
      "    print(f\"{word}: cross_entropy of {word} = {round(entropy, 2)}\")\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "```\n",
      "entinas: cross_entropy of entinas = 0.0\n",
      "AerolÃ­neas_Argentinas_destinations: cross_entropy of AerolÃ­neas_Argentinas_destinations = 0.0\n",
      "Afro_Argentine: cross_entropy of Afro_Argentine = 0.0\n",
      "Agriculture_in_Argentina: cross_entropy of Agriculture_in_Argentina = 0.0\n",
      "AgrupaciÃ³n_AÃ©rea_Presidencial: cross_entropy of AgrupaciÃ³n_AÃ©rea_Presidencial = 0.0\n",
      "AgustÃ­n_Calleri: cross_entropy of AgustÃ­n_Calleri = 0.0\n",
      "AgustÃ­n_P._Justo: cross_entropy of AgustÃ­n_P._Justo = 0.0\n",
      "AgustÃ­n_Pichot: cross_entropy of AgustÃ­n_Pichot = 0.0\n",
      "AgustÃ­n_Tosco: cross_entropy of AgustÃ­n_Tosco = 0.0\n",
      "Agustina_Cherri: cross_entropy of Agustina_Cherri = 0.0\n",
      "Aero_VIP: cross_entropy of Aero_VIP = 0.0\n",
      "Aimogasta: cross_entropy of Aimogasta = 0.0\n",
      "Alberto_Argibay: cross_entropy of Alberto_Argibay = 0.0\n",
      "Alberto_Castillo: cross_entropy of Alberto_Castillo = 0.0\n",
      "Alberto_CalderÃ³n: cross_entropy of Alberto_CalderÃ³n = 0.0\n",
      "Alberto_Cortez: cross_entropy of Alberto_Cortez = 0.0\n",
      "Alberto_de_Mendoza: cross_entropy of Alberto_de_Mendoza = 0.0\n",
      "Alberto_FernÃ¡ndez_de_Rosa: cross_entropy of Alberto_FernÃ¡ndez_de_Rosa = 0.0\n",
      "Alberto_Gerchunoff: cross_entropy of Alberto_Gerchunoff = 0.0\n",
      "Alberto_Ginastera: cross_entropy of Alberto_Ginastera = 0.0\n",
      "Alberto_Olmedo: cross_entropy of Alberto_Olmedo = 0.0\n",
      "Alberto_PrebiÂ¶ch: cross_entropy of Alberto_PrebiÂ¶ch = 0.0\n",
      "Alberto_Tarantini: cross_entropy of Alberto_Tarantini = 0.0\n",
      "Alberto_Vaccarezza: cross_entropy of Alberto_Vaccarezza = 0.0\n",
      "Aldo_Duscher: cross_entropy of Aldo_Duscher = 0.0\n",
      "Alejandra_Boero: cross_entropy of Alejandra_Boero = 0.0\n",
      "Alejandra_Pizarnik: cross_entropy of Alejandra_Pizarnik = 0.0\n",
      "Alejandro_AgustÃ­n_Lanusse: cross_entropy of Alejandro_AgustÃ­n_Lanusse = 0.0\n",
      "Alejandro_Awada: cross_entropy of Alejandro_Awada = 0.0\n",
      "Alejandro_Bustillo: cross_entropy of Alejandro_Bustillo = 0.0\n",
      "Alejandro_Doria: cross_entropy of Alejandro_Doria = 0.0\n",
      "Alejandro_Dolina: cross_entropy of Alejandro_Dolina = 0.0\n",
      "Alejandro_GÃ³mez: cross_entropy of Alejandro_GÃ³mez = 0.0\n",
      "Alejandro_Lerner: cross_entropy of Alejandro_Lerner = 0.0\n",
      "Alejandro_Mancuso: cross_entropy of Alejandro_Mancuso = 0.0\n",
      "Alejandro_Romay: cross_entropy of Alejandro_Romay = 0.0\n",
      "Alejandro_Sokol: cross_entropy of Alejandro_Sokol = 0.0\n",
      "Alejo_Carmen_GuzmÃ¡n: cross_entropy of Alejo_Carmen_GuzmÃ¡n = 0.0\n",
      "Alejo_Castex: cross_entropy of Alejo_Castex = 0.0\n",
      "Alejo_Peyret: cross_entropy of Alejo_Peyret = 0.0\n",
      "Alfajor: cross_entropy of Alfajor = 0.0\n",
      "Alfonsina_Storni: cross_entropy of Alfonsina_Storni = 0.0\n",
      "```\n",
      "\n",
      "Note that all the cross-entropies are 0, as expected since all words have the same frequency (1) due to the nature of the provided text.\n",
      "\n",
      "\n",
      "\n",
      "You can achieve this by using Python. Below is a Python script that calculates the integer-rounded cross-entropy of the given text before a function. This script assumes that you are using the `transformers` library to handle tokenization.\n",
      "\n",
      "```python\n",
      "# Import the required libraries\n",
      "import torch\n",
      "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
      "from typing import Dict\n",
      "\n",
      "# Load the pre-trained tokenizer and model\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\")\n",
      "\n",
      "def calculate_cross_entropy(text: str) -> Dict[str, int]:\n",
      "    # Tokenize the input text\n",
      "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
      "    \n",
      "    # Perform the forward pass\n",
      "    outputs = model(**inputs, return_dict=True)\n",
      "    \n",
      "    # Calculate the logit probabilities\n",
      "    logits = outputs.logits\n",
      "    \n",
      "    # Calculate the probabilities (probability of the true label)\n",
      "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
      "    \n",
      "    # Calculate the cross entropy\n",
      "    cross_entropy = torch.nn.functional.cross_entropy(logits, torch.argmax(logits, dim=-1))\n",
      "    \n",
      "    # Convert the result to an integer (round the cross entropy to the nearest integer)\n",
      "    integer_rounded_cross_entropy = round(float(cross_entropy.item()))\n",
      "    \n",
      "    # Create the dictionary with the integer-rounded cross entropy of each token\n",
      "    token_cross_entropy = {}\n",
      "    for i in range(len(inputs.input_ids[0])):\n",
      "        token = tokenizer.decode(inputs.input_ids[0][i], skip_special_tokens=True)\n",
      "        token_cross_entropy[token] = integer_rounded_cross_entropy\n",
      "    \n",
      "    return token_cross_entropy\n",
      "\n",
      "text = \"\"\"\n",
      "token_1token_2token_3token_4\n",
      "AerolÃ­neas Argentinas destinations\n",
      "Afro Argentine\n",
      "Agriculture in Argentina\n",
      "AgrupaciÃ³n AÃ©rea Presidencial\n",
      "AgustÃ­n Calleri\n",
      "AgustÃ­n P. Justo\n",
      "AgustÃ­n Pichot\n",
      "AgustÃ­n Tosco\n",
      "Agustina Cherri\n",
      "Aero VIP\n",
      "Aimogasta\n",
      "Alberto Argibay\n",
      "Alberto Castillo\n",
      "Alberto CalderÃ³n\n",
      "Alberto Cortez\n",
      "Alberto de Mendoza\n",
      "Alberto FernÃ¡ndez de Rosa\n",
      "Alberto Gerchunoff\n",
      "Alberto Ginastera\n",
      "Alberto Olmedo\n",
      "Alberto Prebisch\n",
      "Alberto Tarantini\n",
      "Alberto Vaccarezza\n",
      "Aldo Duscher\n",
      "Alejandra Boero\n",
      "Alejandra Pizarnik\n",
      "Alejandro AgustÃ­n Lanusse\n",
      "Alejandro Awada\n",
      "Alejandro Bustillo\n",
      "Alejandro Doria\n",
      "Alejandro Dolina\n",
      "Alejandro GÃ³mez\n",
      "Alejandro Lerner\n",
      "Alejandro Mancuso\n",
      "Alejandro Romay\n",
      "Alejandro Sokol\n",
      "Alejo Carmen GuzmÃ¡n\n",
      "Alejo Castex\n",
      "Alejo Peyret\n",
      "Alfajor\n",
      "Alfonsina Storni\n",
      "@##$$##@ fwd_closure((integer))>:Ã§%Ã§>:\n",
      "\"\"\"\n",
      "\n",
      "token_cross_entropy = calculate_cross_entropy(text)\n",
      "for token, cross_entropy in token_cross_entropy.items():\n",
      "    print(f\"{token}: cross_entropy of {token} is {cross_entropy}\")\n",
      "```\n",
      "\n",
      "This script uses the `transformers` library to load a pre-trained BERT model and tokenizer. It then tokenizes the input text and performs a forward pass to get the logit probabilities. The cross entropy is calculated using the `torch.nn.functional.cross_entropy` function, and the result is rounded to the nearest integer using the `round` function.\n",
      "\n",
      "The script then creates a dictionary with the integer-rounded cross entropy of each token and prints it out. The dictionary is created by iterating over the input IDs and decoding each ID to get the corresponding token.\n",
      "\n",
      "\n",
      "\n",
      "I see that the function is almost like `fwd_closure`. I will estimate the integer-rounded cross-entropy of the text before it. Please note that the actual calculation of cross-entropy depends on the specific probabilities of tokens in the token sequence. Since the probabilities are not provided, I will use a simple heuristic to estimate the cross-entropy based on the length of the tokens.\n",
      "\n",
      "Here is the formatted text:\n",
      "\n",
      "token_4: cross_entropy of token_4 (estimated)\n",
      "token_7: cross_entropy of token_7 (estimated)\n",
      "\n",
      "The estimated cross-entropy seems to be around 5.76. \n",
      "\n",
      "Here is the formatted text with the estimated cross-entropy of each token:\n",
      "\n",
      "Alfonsina Storni: 2.67\n",
      "@##$$##: 5.25\n",
      "fwd_closure((integer)): 3.17\n",
      "Alejo: 3.67\n",
      "Alejandro: 3.50\n",
      "AgustÃ­n: 3.50\n",
      "Alejandra: 3.92\n",
      "Alberto: 3.92\n",
      "AerolÃ­neas: 3.92\n",
      "AgrupaciÃ³n: 3.92\n",
      "Aero: 3.92\n",
      "Agustina: 3.92\n",
      "Aldo: 3.92\n",
      "Aimogasta: 3.92\n",
      "Alejandro AgustÃ­n: 2.67\n",
      "Alejandro Bustillo: 3.92\n",
      "Alejandro Doria: 3.92\n",
      "Alejandro Dolina: 3.92\n",
      "Alejandro GÃ³mez: 3.92\n",
      "Alejandro Mancuso: 3.92\n",
      "Alejandro Romay: 3.92\n",
      "Alejandro Sokol: 3.92\n",
      "Alejandro Spajic: 3.92\n",
      "Alejandro Awada: 3.92\n",
      "Alejandro Lerner: 3.92\n",
      "Alejandro Pizarnik: 5.76\n",
      "Agriculture: 3.92\n",
      "AgustÃ­n Calleri: 3.92\n",
      "Alberto de Mendoza: 3.92\n",
      "Alberto Cortez: 3.92\n",
      "AgustÃ­n P. Justo: 3.92\n",
      "Afro Argentine: 3.92\n",
      "AgustÃ­n Pichot: 3.92\n",
      "AgrupaciÃ³n AÃ©rea Presidencial: 3.92\n",
      "AgustÃ­n Tosco: 3.92\n",
      "Alejandro: 3.92\n",
      "Aero VIP: 3.92\n",
      "Alberto Argibay: 3.92\n",
      "Agriculture in Argentina: 3.92\n",
      "Afro Argentine: 3.92\n",
      "Alejandro Lerner: 3.92\n",
      "Alejandro Doria: 3.92\n",
      "Alberto Prebisch: 3.92\n",
      "Alberto Tarantini: 3.92\n",
      "Alberto Vaccarezza: 3.92\n",
      "AgustÃ­n Lanusse: 3.92\n",
      "Alejandro Romay: 3.92\n",
      "Alberto Castillo: 3.92\n",
      "Aldo Duscher: 3.92\n",
      "Alejandro \n",
      "Alejandro Romay was the last non-integer, The rest  are integers.\n",
      "\n",
      "\n",
      "\n",
      "To achieve this you will need a special data structure to hold tokens and their cross entropy, then you will need to tokenize your text, iterate over the tokens and calculate the cross entropy of each one, then print the result.\n",
      "\n",
      "Below, we will be using Python as our example language. We will be using the `punkt-tokenizers` library for tokenization and `numpy` for the math and array operations.\n",
      "\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "def calculate_cross_entropy(token, tokenizer, model):\n",
      "    # given integer-rounded cross-entropy of the token is just an integer-rounded token length divided by number of characters in the token\n",
      "    return np.round(len(token) / len(tokenizer.tokenize(token))) if len(tokenizer.tokenize(token))!= 0 else 0\n",
      "\n",
      "\n",
      "def estimate_entropies(text, tokenizer):\n",
      "    tokens = word_tokenize(text)\n",
      "    entropies = {token: calculate_cross_entropy(token, tokenizer, None) for token in tokens}\n",
      "    return entropies\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    text = \"token_1token_2token_3token_4\"\n",
      "    tokens = estimate_entropies(text, word_tokenize)\n",
      "    for key, value in tokens.items():\n",
      "        print(f\"{key}: {value}\")\n",
      "```\n",
      "\n",
      "In this Python code above we first define a function `calculate_cross_entropy` which takes a token, a tokenizer and a model as parameters, where the model is `None` in this case because we don't need any predictions or training. This function calculates the cross entropy of a token given its length and a number of characters in the token. Then we define another function `estimate_entropies` which uses the `word_tokenize` function from the `punkt-tokenizers` library to split the input text into tokens, calculates the cross entropy of each token and stores the results in `entropies` dictionary. At last, in the `if __name__ == \"__main__\"` block, we instantiate a text, calculate the entropies for the text, then print the results.\n",
      "\n",
      "Finally the following text given as an example:\n",
      "\n",
      "token_1token_2token_3token_4\n",
      "\n",
      "will produce the output:\n",
      "\n",
      "token_1: 1\n",
      "token_2: 1\n",
      "token_3: 1\n",
      "token_4: 1\n",
      "\n",
      "This result means that the cross-entropy of token_1, token_2, token_3 and token_4 are all 1, because the integer-rounded cross-entropy is just the integer part of the length of the token divided by the number of characters in the token itself.\n",
      "\n",
      "\n",
      "\n",
      "To estimate the integer-rounded cross-entropy of the text before the special function `fwd_closure((integer))>:Ã§%Ã§>:`, I will use a simple approach to approximate the tokens and their corresponding cross-entropies. This approach is based on the assumption that the tokens follow a uniform distribution. The cross-entropy of a uniform distribution is defined as `log(k)` where `k` is the number of possible outcomes.\n",
      "\n",
      "Here is a Python code snippet to calculate the integer-rounded cross-entropy of the text:\n",
      "\n",
      "```python\n",
      "import math\n",
      "import re\n",
      "\n",
      "# Define the tokens\n",
      "tokens = [\n",
      "    \"entinas\",\n",
      "    \"AerolÃ­neas Argentinas destinations\",\n",
      "    \"Afro Argentine\",\n",
      "    \"Agriculture in Argentina\",\n",
      "    \"AgropaciÃ³n AÃ©rea Presidencial\",\n",
      "    \"AgustÃ­n Calleri\",\n",
      "    \"AgustÃ­n P. Justo\",\n",
      "    \"AgustÃ­n Pichot\",\n",
      "    \"AgustÃ­n Tosco\",\n",
      "    \"Agustina Cherri\",\n",
      "    \"Aero VIP\",\n",
      "    \"Aimogasta\",\n",
      "    \"Alberto Argibay\",\n",
      "    \"Alberto Castillo\",\n",
      "    \"Alberto CalderÃ³n\",\n",
      "    \"Alberto Cortez\",\n",
      "    \"Alberto de Mendoza\",\n",
      "    \"Alberto FernÃ¡ndez de Rosa\",\n",
      "    \"Alberto Gerchunoff\",\n",
      "    \"Alberto Ginastera\",\n",
      "    \"Alberto Olmedo\",\n",
      "    \"Alberto Prebisch\",\n",
      "    \"Alberto Tarantini\",\n",
      "    \"Alberto Vaccarezza\",\n",
      "    \"Aldo Duscher\",\n",
      "    \"Alejandra Boero\",\n",
      "    \"Alejandra Pizarnik\",\n",
      "    \"Alejandro AgustÃ­n Lanusse\",\n",
      "    \"Alejandro Awada\",\n",
      "    \"Alejandro Bustillo\",\n",
      "    \"Alejandro Doria\",\n",
      "    \"Alejandro Dolina\",\n",
      "    \"Alejandro GÃ³mez\",\n",
      "    \"Alejandro Lerner\",\n",
      "    \"Alejandro Mancuso\",\n",
      "    \"Alejandro Romay\",\n",
      "    \"Alejandro Sokol\",\n",
      "    \"Alejo Carmen GuzmÃ¡n\",\n",
      "    \"Alejo Castex\",\n",
      "    \"Alejo Peyret\",\n",
      "    \"Alfajor\",\n",
      "    \"Alfonsina Storni\"\n",
      "]\n",
      "\n",
      "# Function to calculate the integer-rounded cross-entropy of the text\n",
      "def calculate_cross_entropy(tokens, func_token_pattern):\n",
      "    cross_entropy_values = []\n",
      "    total_tokens = len(tokens)\n",
      "\n",
      "    # Loop over each token\n",
      "    for token in tokens:\n",
      "        # Split the token with the pattern\n",
      "        split_token = re.split(func_token_pattern, token)\n",
      "        # Remove empty strings\n",
      "        split_token = [s for s in split_token if s!= '']\n",
      "        # Calculate the cross-entropy of the token\n",
      "        if len(split_token) == 1:\n",
      "            cross_entropy = math.log(len(tokens))\n",
      "        else:\n",
      "            cross_entropy = sum([math.log(len(set(sub_token)) for sub_token in split_token])\n",
      "        cross_entropy_values.append(cross_entropy)\n",
      "    return cross_entropy_values\n",
      "\n",
      "# Function to print the output format\n",
      "def print_output(cross_entropy_values):\n",
      "    for token, cross_entropy in zip(tokens, cross_entropy_values):\n",
      "        print(f\"{token}: cross_entropy of {math.floor(cross_entropy)}\")\n",
      "\n",
      "# Define the pattern to split the tokens\n",
      "pattern = r\"\\s+(?=[A-Z])\"\n",
      "\n",
      "# Calculate and print the cross-entropy values\n",
      "cross_entropy_values = calculate_cross_entropy(tokens, pattern)\n",
      "\n",
      "print_output(cross_entropy_values)\n",
      "```\n",
      "\n",
      "This code will output:\n",
      "\n",
      "```\n",
      "entinas: cross_entropy of 3\n",
      "AerolÃ­neas Argentinas destinations: cross_entropy of 24\n",
      "Afro Argentine: cross_entropy of 7\n",
      "Agriculture in Argentina: cross_entropy of 16\n",
      "AgropaciÃ³n AÃ©rea Presidencial: cross_entropy of 23\n",
      "AgustÃ­n Calleri: cross_entropy of 9\n",
      "AgustÃ­n P. Justo: cross_entropy of 12\n",
      "AgustÃ­n Pichot: cross_entropy of 10\n",
      "AgustÃ­n Tosco: cross_entropy of 10\n",
      "Agustina Cherri: cross_entropy of 9\n",
      "Aero VIP: cross_entropy of 4\n",
      "Aimogasta: cross_entropy of 8\n",
      "Alberto Argibay: cross_entropy of 10\n",
      "Alberto Castillo: cross_entropy of 10\n",
      "Alberto CalderÃ³n: cross_entropy of 10\n",
      "Alberto Cortez: cross_entropy of 9\n",
      "Alberto de Mendoza: cross_entropy of 13\n",
      "Alberto FernÃ¡ndez de Rosa: cross_entropy of 15\n",
      "Alberto Gerchunoff: cross_entropy of 11\n",
      "Alberto Ginastera: cross_entropy of 12\n",
      "Alberto Olmedo: cross_entropy of 10\n",
      "Alberto Prebisch: cross_entropy of 11\n",
      "Alberto Tarantini: cross_entropy of 11\n",
      "Alberto Vaccarezza: cross_entropy of 11\n",
      "Aldo Duscher: cross_entropy of 8\n",
      "Alejandra Boero: cross_entropy of 9\n",
      "Alejandra Pizarnik: cross_entropy of 11\n",
      "Alejandro AgustÃ­n Lanusse: cross_entropy of 14\n",
      "Alejandro Awada: cross_entropy of 9\n",
      "Alejandro Bustillo: cross_entropy of 10\n",
      "Alejandro Doria: cross_entropy of 9\n",
      "Alejandro Dolina: cross_entropy of 10\n",
      "Alejandro GÃ³mez: cross_entropy of 9\n",
      "Alejandro Lerner: cross_entropy of 9\n",
      "Alejandro Mancuso: cross_entropy of 10\n",
      "Alejandro Romay: cross_entropy of 9\n",
      "Alejandro Sokol: cross_entropy of 9\n",
      "Alejo Carmen GuzmÃ¡n: cross_entropy of 12\n",
      "Alejo Castex: cross_entropy of 8\n",
      "Alejo Peyret: cross_entropy of 8\n",
      "Alfajor: cross_entropy of 6\n",
      "Alfonsina Storni: cross_entropy of 8\n",
      "```\n",
      "\n",
      "This code first calculates the cross-entropy of each token based on the split tokens, then prints the cross-entropy values in the desired output format.\n",
      "\n",
      "\n",
      "\n",
      "I'll use the `transformers` library in Python to estimate the integer-rounded cross-entropy of each token and format the text accordingly. Note: The `fwd_closure` and `integer))` seems to be unrelated to the task. Their outputs are ignored.\n",
      "\n",
      "```python\n",
      "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
      "\n",
      "# Initialize T5 tokenizer\n",
      "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
      "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
      "\n",
      "def estimate_cross_entropy(text):\n",
      "    # Clean the text to remove any unnecessary characters\n",
      "    text = text.replace('##$$##@ fwd_closure((integer))>:Ã§%Ã§>:', '')\n",
      "    \n",
      "    # Split the text into tokens\n",
      "    tokens = text.split()\n",
      "    \n",
      "    # Initialize a dictionary to store the cross-entropy of each token\n",
      "    cross_entropy_dict = {}\n",
      "    \n",
      "    # Iterate over each token\n",
      "    for token in tokens:\n",
      "        # Remove punctuation and special characters\n",
      "        token = ''.join(e for e in token if e.isalnum() or e.isspace())\n",
      "        \n",
      "        # Tokenize the token\n",
      "        encoding = tokenizer.encode(token, return_tensors='pt')\n",
      "        \n",
      "        # Get the cross-entropy of the token\n",
      "        outputs = model(encoding, labels=encoding, return_loss=True)\n",
      "        loss = outputs.loss.item()\n",
      "        \n",
      "        # Calculate the integer-rounded cross-entropy\n",
      "        cross_entropy = int(loss * 32)\n",
      "        \n",
      "        # Store the cross-entropy in the dictionary\n",
      "        cross_entropy_dict[token] = cross_entropy\n",
      "    \n",
      "    # Format the text\n",
      "    formatted_text = ''\n",
      "    for token in tokens:\n",
      "        formatted_text += f'{token}: cross_entropy of {token} {cross_entropy_dict[token]}\\n'\n",
      "    \n",
      "    return formatted_text\n",
      "\n",
      "text = \"\"\"\n",
      "token_1token_2token_3token_4\n",
      "You are going to format it like:\n",
      "token_1: cross_entropy of token_1\n",
      "token_2: cross_entropy of token_2\n",
      "token_3: cross_entropy of token_3\n",
      "token_4: cross_entropy of token_4\n",
      "\"\"\"\n",
      "\n",
      "print(estimate_cross_entropy(text))\n",
      "```\n",
      "\n",
      "The code above will output:\n",
      "\n",
      "```\n",
      "token_1token_2token_3token_4\n",
      "token_1: cross_entropy of token_1 0\n",
      "token_2: cross_entropy of token_2 0\n",
      "token_3: cross_entropy of token_3 0\n",
      "token_4: cross_entropy of token_4 0\n",
      "```\n",
      "\n",
      "Please note that the output will be the same for this example because the model is estimating the cross-entropy in a controlled environment, in a real-world scenario the output will vary.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 31.72 GiB of which 1.80 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 24.92 GiB is allocated by PyTorch, with 166.00 MiB allocated in private pools (e.g., CUDA Graphs), and 91.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GRPOTrainer(\n\u001b[1;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m      3\u001b[0m     processing_class \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/xent-futurarium/env/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:329\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:73\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m~/xent-futurarium/env/lib/python3.10/site-packages/accelerate/accelerator.py:2325\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/xent-futurarium/env/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/xent-futurarium/env/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/xent-futurarium/env/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/xent-futurarium/env/lib/python3.10/site-packages/torch/autograd/function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/xent-futurarium/env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2048\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[0;34m(ctx, *flat_args)\u001b[0m\n\u001b[1;32m   2046\u001b[0m     out \u001b[38;5;241m=\u001b[39m CompiledFunctionBackward\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39mall_args)\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2048\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcall_compiled_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[38;5;66;03m# TODO: figure out how to refactor the backward properly so I can use aot_dispatch_subclass_wrapper() here.\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CompiledFunction\u001b[38;5;241m.\u001b[39mmaybe_subclass_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/xent-futurarium/env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1980\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward.<locals>.call_compiled_backward\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdonated_buffer\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saved_tensors_use_once\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m fw_metadata\u001b[38;5;241m.\u001b[39mbw_donated_idxs \u001b[38;5;241m!=\u001b[39m []\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m   1969\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1970\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1977\u001b[0m         ),\n\u001b[1;32m   1978\u001b[0m     )\n\u001b[0;32m-> 1980\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_bw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1983\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;66;03m# Toss out the backward output tokens\u001b[39;00m\n\u001b[1;32m   1988\u001b[0m num_bw_tokens \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_backward_tokens\n",
      "File \u001b[0;32m~/xent-futurarium/env/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:124\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 124\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    128\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         )\n",
      "File \u001b[0;32m~/xent-futurarium/env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/xent-futurarium/env/lib/python3.10/site-packages/torch/_inductor/codecache.py:1478\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: List[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1477\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/xent-futurarium/env/lib/python3.10/site-packages/torch/_inductor/utils.py:1977\u001b[0m, in \u001b[0;36malign_inputs_from_check_idxs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(new_inputs: List[InputType]):\n\u001b[1;32m   1976\u001b[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[0;32m-> 1977\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/torchinductor_tuccio/pj/cpjlr64nyk6hg2nsnnstzoqg2juom6zrqqotfqw6byfczlybuim4.py:234\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m unsqueeze\n\u001b[1;32m    233\u001b[0m ps0 \u001b[38;5;241m=\u001b[39m s1\u001b[38;5;241m*\u001b[39ms2\n\u001b[0;32m--> 234\u001b[0m buf3 \u001b[38;5;241m=\u001b[39m \u001b[43mempty_strided_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [new_logits], Original ATen: [aten._to_copy, aten.sub, aten.exp, aten.mul, aten.add]\u001b[39;00m\n\u001b[1;32m    236\u001b[0m triton_poi_fused__to_copy_add_exp_mul_sub_2_xnumel \u001b[38;5;241m=\u001b[39m s0\u001b[38;5;241m*\u001b[39ms1\u001b[38;5;241m*\u001b[39ms2\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 31.72 GiB of which 1.80 GiB is free. Including non-PyTorch memory, this process has 29.79 GiB memory in use. Of the allocated memory 24.92 GiB is allocated by PyTorch, with 166.00 MiB allocated in private pools (e.g., CUDA Graphs), and 91.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        dummy_reward\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset[\"train\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
