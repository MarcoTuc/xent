{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from xent.tasks import Closure\n",
    "from xent.models import M\n",
    "from xent.lang import X\n",
    "from xent.dataprocessing import Wikipedia\n",
    "from xent.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M(\"gpt2\", \"M0\", base=\"base\")\n",
    "checker_model = M(\"gpt2\", \"M1-zero\", base=\"closure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd70c9fdf04f4e7a87b8b1220cb946a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_generator = Wikipedia(split=0.8)\n",
    "get_test_sample = corpus_generator.get_random_test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Closure(model)\n",
    "synth = task.generate(get_test_sample, space=\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = task.find_xstring(synth[0], X.xreturn)\n",
    "tasktokens = model.tokenize(synth[0]).input_ids.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[11207]], device='cuda:2'), 'attention_mask': tensor([[1]], device='cuda:2')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut = cut + 5\n",
    "\n",
    "toks = tasktokens[cut::]\n",
    "check = model.detokenize(toks, mode=\"tensor\")\n",
    "\n",
    "model.tokenize(\".:\")\n",
    "\n",
    "# .: is out there seeking to destroy your whole career as a computer scientist indeed \n",
    "# it will break the slicing of your tensor and it will probably also steal your car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [\" 0\",\" 1\",\" 2\",\" 3\",\" 4\",\" 5\",\" 6\",\" 7\",\" 8\",\" 9\",\" 10\",\" 11\",\" 12\",\" 13\",\" 14\",\" 15\",\" 16\",\" 17\",\" 18\",\" 19\",\" 20\"]\n",
    "numtoks = torch.tensor([model.tokenize(num).input_ids for num in numbers]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9 1 9 11 0 0 0 14 4 3 4 5 3 4 5 0 11 3 3 1 5 7 7 3 4 0 3 5 2 1 4 0 1 8 7 5 2 3 9 3 0 1 0 1 1 0 6 3 2 16 3 4 1 4 19 3 4 3 2 4 2 0 2 5 4 3 4 4 0 1 4 2 1 9 10 3 8 2 6 4 6 2 6 4 4 1 5 0 2 7 3 4 5 0 1 4 4 10 1 9 1 10 4 0 4 6 1 0 3 0 8 2 1 2 1 2 5 0 1 1 6 4 8 2 3 10 2 7 6 7 1 3 4 0 4 1 0 2 9 8 2 1 7 8 1 0 5 3 4 0 1 0 2 5 2 2 7 0 0 1 19 2 5 4 7 4 4 20 3 1 2 1 2 0 0 7 1 2 0 6 1 1 0 6 2 0 0 1 8 2 4 3 4 11 1 3 6 7 0 3 4 7 5 11 1 0\n"
     ]
    }
   ],
   "source": [
    "# Check if each element in toks is in numtoks\n",
    "is_number = torch.tensor([tok in numtoks for tok in toks])\n",
    "# Get indices where is_number is True\n",
    "number_positions = torch.where(is_number)[0]\n",
    "print(model.detokenize(toks[number_positions], mode=\"tensor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasktokens = tasktokens.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "checker_model.model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = checker_model.model(tasktokens).logits\n",
    "    loss = F.cross_entropy(logits[0, cut:-1], tasktokens[0, cut+1:], reduction=\"none\")\n",
    "    # Get the predicted token probabilities\n",
    "    probs = F.softmax(logits[0, cut:-1], dim=-1)\n",
    "    # Get the indices of tokens with highest probabilities\n",
    "    highest_prob_tokens = torch.argmax(probs, dim=-1)\n",
    "    # Convert to list for easier inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([801]) torch.Size([801]) torch.Size([801])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 513,  860,  657,  860,  513,  657,  657,  657,  657,  657,  657,  657,\n",
       "         604,  718,  604,  362,  657,  718,  362,  604,  352,  642,  767,  718,\n",
       "         513,  604,  657,  604,  604,  352,  352,  604,  657,  362,  807,  352,\n",
       "         604,  352,  362,  807,  657,  657,  352,  657,  352,  352,  657,  718,\n",
       "         642,  362, 1467,  513,  513,  352,  678,  604,  513,  352,  362,  362,\n",
       "         362,  352,  352,  362,  604,  604,  352,  362,  513,  352,  362,  362,\n",
       "         362,  362,  807,  362,  362,  718,  352,  604,  362,  604,  352,  604,\n",
       "         352,  604,  352,  362,  657,  362,  513,  362,  604,  657,  657,  352,\n",
       "         807,  362,  604,  362,  807,  352,  807,  657,  352,  604,  642,  352,\n",
       "         657,  513,  362,  718,  352,  362,  352,  362,  513,  642,  657,  352,\n",
       "         362,  362,  604,  604,  352,  362,  807,  352,  718,  807,  718,  657,\n",
       "         807,  642,  657,  604,  718,  657,  352,  807,  718,  657,  352,  767,\n",
       "         604,  657,  657,  604,  362,  604,  657,  352,  657,  362,  362,  362,\n",
       "         513,  604,  657,  657,  657,  678,  657,  642,  604,  718,  604, 1160,\n",
       "         767,  362,  657,  513,  657, 2242,  657,  657,  807,  657,  657,  657,\n",
       "         352,  604,  657,  657,  362,  352,  657, 1679,  352,  807,  657,  657,\n",
       "         657,  657,  807,  657,  352,  718,  807,  657,  362,  604,  362, 1367,\n",
       "         604,  657], device='cuda:2')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_toks = toks[1:]\n",
    "pred_toks = highest_prob_tokens\n",
    "loss = loss\n",
    "\n",
    "print(cut_toks.shape, pred_toks.shape, loss.shape)\n",
    "\n",
    "is_number = torch.tensor([tok in numtoks for tok in cut_toks])\n",
    "numpos = torch.where(is_number)[0]\n",
    "\n",
    "# for origin, pred, l in zip(cut_toks[numpos], pred_toks[numpos], loss[numpos]):\n",
    "#     print(f\"original: {model.detokenize(origin, mode='tensor')} \\t| predicted: {model.detokenize(pred, mode='tensor')} \\t| loss: {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try another approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cut_toks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcut_toks\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cut_toks' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
