{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from xent import M\n",
    "from xent.datasets import DataProcessor\n",
    "from xent.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer path: /rcp/marco/models/base/gpt2-xl/M0\n",
      "model path: /rcp/marco/models/base/gpt2-xl/M0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e76e40e35074732896decf2428d7912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = M(\"base\", \"gpt2-xl\", \"M0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e33fb774ae4599a54ef607b63e443a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\n",
    "    \"wikipedia\", \n",
    "    \"20220301.en\", \n",
    "    trust_remote_code=True,\n",
    "    )[\"train\"]\n",
    "\n",
    "# this is needed to reproduce how I separated the data in the parallel_parallel\n",
    "# generator, since I want to pre-train the model on that corpus I'm splitting it \n",
    "# in the same way so we have a reproducible thing. \n",
    "# the \"shards\" parameter is needed to define to which parallel_parallel checkpoint \n",
    "# one wants to reproduce the corpus dataset. \n",
    "# For now I aim to pre-train on 90 checkpoints. \n",
    "n_shards = 600\n",
    "shards = 90\n",
    "data_1 = data.shard(num_shards=3, index=0)\n",
    "data_2 = data.shard(num_shards=3, index=1)\n",
    "data_3 = data.shard(num_shards=3, index=2)\n",
    "finaldata = []\n",
    "for shard in range(shards):\n",
    "    finaldata.append(data_1.shard(num_shards=n_shards, index=shard))\n",
    "    finaldata.append(data_2.shard(num_shards=n_shards, index=shard))\n",
    "    finaldata.append(data_3.shard(num_shards=n_shards, index=shard))\n",
    "finaldata = datasets.concatenate_datasets(finaldata)\n",
    "finaldata = finaldata.train_test_split(test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:06<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# here define how the new dataset has to be saved \n",
    "shards_to_save = 50000\n",
    "base_save_dir = \"wiki90-tok\"\n",
    "\n",
    "# tokenize and save the training set\n",
    "splits = [\"train\", \"test\"]\n",
    "for split in splits:\n",
    "    for sh in tqdm(range(shards_to_save)):\n",
    "        tensor_to_save = torch.LongTensor([]).to(device)\n",
    "        dshard = finaldata[split].shard(num_shards=shards_to_save, index=sh)\n",
    "        for el in tqdm(dshard, leave=False):\n",
    "            tensor_to_save = torch.cat([\n",
    "                tensor_to_save, \n",
    "                model.tokenize(el[\"text\"]).input_ids, \n",
    "                torch.LongTensor([model.tokenizer.eos_token_id]).unsqueeze(0).to(device)\n",
    "                ], dim=-1)  \n",
    "        break\n",
    "    break\n",
    "        # save_dir = os.path.join(base_save_dir, split)\n",
    "        # save_name = f\"{split}_{str(sh).zfill(4)}\"\n",
    "        # DataProcessor.pickle_dump(\n",
    "        #     tensor_to_save,\n",
    "        #     save_dir,\n",
    "        #     save_name\n",
    "        # )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
