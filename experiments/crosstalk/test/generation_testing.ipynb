{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"./../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from xent.tasks import Closure\n",
    "from xent import M, T\n",
    "from xent.datasets import SynthProcessor\n",
    "from xent.lang import X\n",
    "from xent.config import *\n",
    "\n",
    "torch.manual_seed(21)\n",
    "torch.set_printoptions(threshold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer path: /rcp/marco/models/base/gpt2/M0\n",
      "model path: /rcp/marco/models/crosstalk-cluster/gpt2/gpt2-base2closure\n"
     ]
    }
   ],
   "source": [
    "model = M(\"crosstalk-cluster\", \"gpt2\", \"gpt2-base2closure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data files:   0%|          | 0/273 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data files: 100%|██████████| 273/273 [01:11<00:00,  3.80it/s]\n"
     ]
    }
   ],
   "source": [
    "data  = SynthProcessor(\n",
    "    base=\"parallel_parallel\",\n",
    "    dataset_name=\"fwd_closure_i\",\n",
    "    # files_to_load=10,\n",
    ")\n",
    "# cut the dataset\n",
    "data.dataset = data.dataset[:3400000]\n",
    "# shuffle the dataset\n",
    "data.dataset = data.dataset[torch.randperm(data.dataset.shape[0])]\n",
    "# train test split\n",
    "train_set, test_set = data.dataset[:3000000], data.dataset[3000000:3000000+400000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = T(model, train_set, test_set, batch_size=10, report_wandb=False, eval_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [15:04<00:00,  9.05s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(100)):\n",
    "    prompt, output, true, tensor, lenprompt = train.gen_in_loop(split=True)\n",
    "    if len(output) == 0:\n",
    "        print(\"heh dude you messed up\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch || :   0%|          | 0/100.0 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4917,   290, 13897,   262,  7659,  5733,  1168,   585,    11,   691,\n",
      "          284,   307,  6572,   416,   262,  7659,  5733, 20260,  8336,  1031,\n",
      "          585,    13,   198, 13711,   291,   622,   417,   357, 13038,  3711,\n",
      "          416,  1526, 13808,   520,  1252,     8,   532,   317,  6715, 16599,\n",
      "           12, 26966,  9234,  2727,   416,  7695,   271, 15918,   422,   257,\n",
      "         4048, 25313,   844,  2884,  5536, 16029,   508, 15445,    82,   257,\n",
      "         5536,  7521, 32680,   326,  3578,   607,   284, 13813,  4970,   290,\n",
      "         4351,   511,  1176,   284,  1854,    11,  5078,  5920,    11,   290,\n",
      "          787,  1854,  2121,   287,  1842,   351,   607,    13, 31679,  2645,\n",
      "         3544, 13711,   291,   622,   417,   287,   257,  7110,   284,   787,\n",
      "          262, 13804,  2121,   287,  1842,   351,   607,   290, 30981,  1577,\n",
      "          511,   412, 25649,  5232,   284,   607,    13,  7945,   852, 11791,\n",
      "          416,  7695,   271, 15918,   290,   370,  3532,    11, 13711,   291,\n",
      "          622,   417,   318,  6572,   416,   262,   350, 49600, 20260,  8336,\n",
      "         1031,   585,   833,   291,  8607, 46689,    13,   198,  1446,   388,\n",
      "         6270,   357, 13038,  3711,   416,  8982, 17233,     8,   532,   317,\n",
      "        13683,    12, 26966,  9234,   508,   318,  1908,   416,  7695,   271,\n",
      "        15918,   290,  4424,   952,   284, 12840,   262, 13804,   287,   262,\n",
      "        16032,  4225, 13528, 12009,  3078,   290,  1234,   606,   319,  4473,\n",
      "          287,   257,   479,   648, 38049,  2184,    11,   691,   284,   307,\n",
      "          198,    31,  2235, 13702,  2235,    31,   277, 16993,    62, 17966,\n",
      "        19510, 41433,  4008, 31175, 16175,     4, 16175, 31175,   198,   290,\n",
      "           25,   604,   198, 13897,    25,   807,   198,   262,    25,   362,\n",
      "          198,  7659,    25,  1105,   198,  5733,    25,   642,   198,  1168,\n",
      "           25,   807,   198,   585,    25,   807,   198,    11,    25,   642,\n",
      "          198,   691,    25,   718,   198,   284,    25,   352,   198,   307,\n",
      "           25,   362,   198,  6572,    25,   604,   198,   416,    25,   352,\n",
      "          198,   262,    25,   352,   198,  7659,    25,   642,   198,  5733,\n",
      "           25,   657,   198, 20260,    25,  1367,   198,  8336,    25,  1105,\n",
      "          198,  1031,    25,   604,   198,   585,    25,   657,   198,    13,\n",
      "           25,   352,   198,   198,    25,   352,   198, 13711,    25,  2808,\n",
      "          198,   291,    25,   718,   198,   622,    25,   642,   198,   417,\n",
      "           25,   362,   198,   357,    25,   604,   198, 13038,    25,   767,\n",
      "          198,  3711,    25,   657,   198,   416,    25,   657,   198,  1526,\n",
      "           25,   642,   198, 13808,    25,   642,   198,   520,    25,   642,\n",
      "          198,  1252,    25,   642,   198,     8,    25,   657,   198,   532,\n",
      "           25,   642,   198,   317,    25,   513,   198,  6715,    25,   807,\n",
      "          198, 16599,    25,   362,   198,    12,    25,   718,   198, 26966,\n",
      "           25,   642,   198,  9234,    25,   718,   198,  2727,    25,   642,\n",
      "          198,   416,    25,   657,   198,  7695,    25,   838,   198,   271,\n",
      "           25,   718,   198, 15918,    25,   718,   198,   422,    25,   718,\n",
      "          198,   257,    25,   513,   198,  4048,    25,   767,   198, 25313,\n",
      "           25,   807,   198,   844,    25,   860,   198,  2884,    25,  1367,\n",
      "          198,  5536,    25,   718,   198, 16029,    25,   860,   198,   508,\n",
      "           25,   718,   198, 15445,    25,   718,   198,    82,    25,   657,\n",
      "          198,   257,    25,   352,   198,  5536,    25,   604,   198,  7521,\n",
      "           25,   860,   198, 32680,    25,   352,   198,   326,    25,   513,\n",
      "          198,  3578,    25,   604,   198,   607,    25,   657,   198,   284,\n",
      "           25,   657,   198, 13813,    25,  1367,   198,  4970,    25,   807,\n",
      "          198,   290,    25,   362,   198,  4351,    25,   807,   198,   511,\n",
      "           25,   362,   198,  1176,    25,   362,   198,   284,    25,   352,\n",
      "          198,  1854,    25,   604,   198,    11,    25,   513,   198,  5078,\n",
      "           25,  1105,   198,  5920,    25,   604,   198,    11,    25,   352,\n",
      "          198,   290,    25,   352,   198,   787,    25,   604,   198,  1854,\n",
      "           25,   513,   198,  2121,    25,   642,   198,   287,    25,   352,\n",
      "          198,  1842,    25,   657,   198,   351,    25,   352,   198,   607,\n",
      "           25,   657,   198,    13,    25,   657,   198, 31679,    25,  1105,\n",
      "          198,  2645,    25,   838,   198,  3544,    25,   838,   198, 13711,\n",
      "           25,   838,   198,   291,    25,   657,   198,   622,    25,   657,\n",
      "          198,   417,    25,   657,   198,   287,    25,   513,   198,   257,\n",
      "           25,   362,   198,  7110,    25,   718,   198,   284,    25,   362,\n",
      "          198,   787,    25,   513,   198,   262,    25,   513,   198, 13804,\n",
      "           25,  1105,   198,  2121,    25,   642,   198,   287,    25,   657,\n",
      "          198,  1842,    25,   657,   198,   351,    25,   657,   198,   607,\n",
      "           25,   352,   198,   290,    25,   362,   198, 30981,    25,   860,\n",
      "          198,  1577,    25,   362,   198,   511,    25,   604,   198,   412,\n",
      "           25,   860,   198, 25649,    25,   642,   198,  5232,    25,   838,\n",
      "          198,   284,    25,   657,   198,   607,    25,   352,   198,    13,\n",
      "           25,   352,   198,  7945,    25,   718,   198,   852,    25,   352,\n",
      "          198, 11791,    25,   860,   198,   416,    25,   657,   198,  7695,\n",
      "           25,   642,   198,   271,    25,   657,   198, 15918,    25,   657,\n",
      "          198,   290,    25,   513,   198,   370,    25,   767,   198,  3532,\n",
      "           25,   807,   198,    11,    25,   352,   198, 13711,    25,   657,\n",
      "          198,   291,    25,   657,   198,   622,    25,   657,   198,   417,\n",
      "           25,   657,   198,   318,    25,   352,   198,  6572,    25,   642,\n",
      "          198,   416,    25,   352,   198,   262,    25,   352,   198,   350,\n",
      "           25,   807,   198, 49600,    25,   767,   198, 20260,    25,   352,\n",
      "          198,  8336,    25,   352,   198,  1031,    25,   657,   198,   585,\n",
      "           25,   657,   198,   833,    25,  1105,   198,   291,    25,   604,\n",
      "          198,  8607,    25,   807,   198, 46689,    25,  1367,   198,    13,\n",
      "           25,   352,   198,   198,    25,   352,   198,  1446,    25,  1596,\n",
      "          198,   388,    25,   604,   198,  6270,    25,   838,   198,   357,\n",
      "           25,   657,   198, 13038,    25,   657,   198,  3711,    25,   657,\n",
      "          198,   416,    25,   657,   198,  8982,    25,   718,   198, 17233,\n",
      "           25,   860,   198,     8,    25,   657,   198,   532,    25,   657,\n",
      "          198,   317,    25,   352,   198, 13683,    25,   860,   198,    12,\n",
      "           25,   362,   198, 26966,    25,   362,   198,  9234,    25,   352,\n",
      "          198,   508,    25,   362,   198,   318,    25,   362,   198,  1908,\n",
      "           25,   718,   198,   416,    25,   362,   198,  7695,    25,   513,\n",
      "          198,   271,    25,   657,   198, 15918,    25,   657,   198,   290,\n",
      "           25,   513,   198,  4424,    25,   838,   198,   952,    25,   604,\n",
      "          198,   284,    25,   657,   198, 12840,    25,   767,   198,   262,\n",
      "           25,   362,   198, 13804,    25,   362,   198,   287,    25,   352,\n",
      "          198,   262,    25,   362,   198, 16032,    25,   838,   198,  4225,\n",
      "           25,   838,   198, 13528,    25,   513,   198, 12009,    25,   657,\n",
      "          198,  3078,    25,   718,   198,   290,    25,   513,   198,  1234,\n",
      "           25,   642,   198,   606,    25,   352,   198,   319,    25,   513,\n",
      "          198,  4473,    25,   657,   198,   287,    25,   513,   198,   257,\n",
      "           25,   362,   198,   479,    25,   860,   198,   648,    25,   604,\n",
      "          198, 38049,    25,   657,   198,  2184,    25,   657,   198,    11,\n",
      "           25,   513,   198,   691,    25,   513,   198,   284,    25,   657,\n",
      "          198,   307,    25,   352,   198, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch || :   1%|          | 1/100.0 [00:00<00:26,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  262,  6026,   701,    86, 21223,  4752, 44417,    13,   198,   198,\n",
      "          464,  6026,   701,    86, 21223,   635, 13112,  1104,   329,   262,\n",
      "         2323,  3386,    11,   351,  3998,   400,  1659,   268,   338,  1610,\n",
      "          494,  1362,    74,   273,   862, 25197,  7348,  8119,  1104, 10566,\n",
      "           13,   383,  7570,   604,   400,  5407,   739, 24081, 27086, 23403,\n",
      "        12769,   550,   663, 10043,  6572,  1474,   347,  2118,    12,    43,\n",
      "          270,   709,  8135,   416, 12585, 10083,    82,   422,   520,    38,\n",
      "         8541,    13,   383, 28757,   287,   347,  2118,    12,    43,   270,\n",
      "          709,  8135,   373,  6572,   416,   281, 21431,  6374, 14918,  5194,\n",
      "           11,  5710,   416,   509,    38,   513,    13,   383,  2679,  5407,\n",
      "        11615,   284,  5529,   262, 16511,   618,   340,   750,  6758,   287,\n",
      "         2207,   343,  8493,  7570, 30648,    13, 18023,    11,   262,  2297,\n",
      "         5407,  6265,   503,   379,  1755,    11,   832, 17332,    13,   554,\n",
      "          262,  1110,    11,  1402,  2628,  6265,   503,    11, 14928,  9725,\n",
      "          290,  3489, 11926,    13,   383,  6026,   701,    86, 21223,  4054,\n",
      "          284,   987, 11600,   780, 39471,  6215,  7728,   262, 31062,    13,\n",
      "         3998,   400,  1659,   268,  4166,   512, 39158, 10815,    26,  6936,\n",
      "        39471,    13,  2399, 25771,  3818,    11,   509,  7878,  1806,    11,\n",
      "         6149,  6026,   701,  2704, 11404,   362,   284,  6129,  6936, 39471,\n",
      "        10566,    11,  1262, 26127,   290,   367,   641, 29232,   367,    82,\n",
      "          198,    31,  2235, 13702,  2235,    31,   277, 16993,    62, 17966,\n",
      "        19510, 41433,  4008, 31175, 16175,     4, 16175, 31175,   198,  6026,\n",
      "           25,  1367,   198,   701,    25,   352,   198,    86,    25,   657,\n",
      "          198, 21223,    25,   657,   198,  4752,    25,   807,   198, 44417,\n",
      "           25,  1105,   198,    13,    25,   604,   198,   198,    25,   362,\n",
      "          198,   198,    25,   657,   198,   464,    25,   362,   198,  6026,\n",
      "           25,   513,   198,   701,    25,   657,   198,    86,    25,   657,\n",
      "          198, 21223,    25,   657,   198,   635,    25,   513,   198, 13112,\n",
      "           25,   718,   198,  1104,    25,   807,   198,   329,    25,   604,\n",
      "          198,   262,    25,   352,   198,  2323,    25,   767,   198,  3386,\n",
      "           25,   362,   198,    11,    25,   513,   198,   351,    25,   604,\n",
      "          198,  3998,    25,  1511,   198,   400,    25,   642,   198,  1659,\n",
      "           25,   657,   198,   268,    25,   352,   198,   338,    25,   513,\n",
      "          198,  1610,    25,   767,   198,   494,    25,   642,   198,  1362,\n",
      "           25,   352,   198,    74,    25,   718,   198,   273,    25,   767,\n",
      "          198,   862,    25,   657,   198, 25197,    25,   838,   198,  7348,\n",
      "           25,   513,   198,  8119,    25,  1105,   198,  1104,    25,   604,\n",
      "          198, 10566,    25,   362,   198,    13,    25,   362,   198,   383,\n",
      "           25,   362,   198,  7570,    25,   767,   198,   604,    25,   860,\n",
      "          198,   400,    25,   352,   198,  5407,    25,   362,   198,   739,\n",
      "           25,   767,   198, 24081,    25,   838,   198, 27086,    25,   657,\n",
      "          198, 23403,    25,   860,   198, 12769,    25,   513,   198,   550,\n",
      "           25,   513,   198,   663,    25,   604,   198, 10043,    25,   604,\n",
      "          198,  6572,    25,   807,   198,  1474,    25,   642,   198,   347,\n",
      "           25,   604,   198,  2118,    25,   362,   198,    12,    25,   362,\n",
      "          198,    43,    25,   352,   198,   270,    25,   352,   198,   709,\n",
      "           25,   657,   198,  8135,    25,   657,   198,   416,    25,   513,\n",
      "          198, 12585,    25,   807,   198, 10083,    25,   642,   198,    82,\n",
      "           25,   513,   198,   422,    25,   604,   198,   520,    25,   767,\n",
      "          198,    38,    25,   860,   198,  8541,    25,   642,   198,    13,\n",
      "           25,   352,   198,   383,    25,   362,   198, 28757,    25,   838,\n",
      "          198,   287,    25,   604,   198,   347,    25,   513,   198,  2118,\n",
      "           25,   657,   198,    12,    25,   352,   198,    43,    25,   657,\n",
      "          198,   270,    25,   657,   198,   709,    25,   657,   198,  8135,\n",
      "           25,   657,   198,   373,    25,   352,   198,  6572,    25,   513,\n",
      "          198,   416,    25,   352,   198,   281,    25,   604,   198, 21431,\n",
      "           25,  1105,   198,  6374,    25,   860,   198, 14918,    25,  1478,\n",
      "          198,  5194,    25,   513,   198,    11,    25,   362,   198,  5710,\n",
      "           25,   767,   198,   416,    25,   352,   198,   509,    25,   642,\n",
      "          198,    38,    25,   642,   198,   513,    25,   642,   198,    13,\n",
      "           25,   362,   198,   383,    25,   362,   198,  2679,    25,   642,\n",
      "          198,  5407,    25,   604,   198, 11615,    25,   860,   198,   284,\n",
      "           25,   352,   198,  5529,    25,   604,   198,   262,    25,   362,\n",
      "          198, 16511,    25,  1105,   198,   618,    25,   767,   198,   340,\n",
      "           25,   362,   198,   750,    25,   642,   198,  6758,    25,   642,\n",
      "          198,   287,    25,   657,   198,  2207,    25,   718,   198,   343,\n",
      "           25,   657,   198,  8493,    25,   657,   198,  7570,    25,   642,\n",
      "          198, 30648,    25,   718,   198,    13,    25,   352,   198, 18023,\n",
      "           25,   860,   198,    11,    25,   362,   198,   262,    25,   352,\n",
      "          198,  2297,    25,   718,   198,  5407,    25,   657,   198,  6265,\n",
      "           25,   718,   198,   503,    25,   604,   198,   379,    25,   604,\n",
      "          198,  1755,    25,   362,   198,    11,    25,   362,   198,   832,\n",
      "           25,   767,   198, 17332,    25,   767,   198,    13,    25,   642,\n",
      "          198,   554,    25,   513,   198,   262,    25,   362,   198,  1110,\n",
      "           25,   718,   198,    11,    25,   352,   198,  1402,    25,   718,\n",
      "          198,  2628,    25,   362,   198,  6265,    25,   718,   198,   503,\n",
      "           25,   362,   198,    11,    25,   513,   198, 14928,    25,   767,\n",
      "          198,  9725,    25,   767,   198,   290,    25,   352,   198,  3489,\n",
      "           25,  1367,   198, 11926,    25,   718,   198,    13,    25,   352,\n",
      "          198,   383,    25,   362,   198,  6026,    25,   362,   198,   701,\n",
      "           25,   657,   198,    86,    25,   657,   198, 21223,    25,   657,\n",
      "          198,  4054,    25,   718,   198,   284,    25,   657,   198,   987,\n",
      "           25,   860,   198, 11600,    25,   657,   198,   780,    25,   860,\n",
      "          198, 39471,    25,   860,   198,  6215,    25,   604,   198,  7728,\n",
      "           25,   807,   198,   262,    25,   362,   198, 31062,    25,   604,\n",
      "          198,    13,    25,   513,   198,  3998,    25,   860,   198,   400,\n",
      "           25,   657,   198,  1659,    25,   657,   198,   268,    25,   657,\n",
      "          198,  4166,    25,   767,   198,   512,    25,  1367,   198, 39158,\n",
      "           25,   352,   198, 10815,    25,   362,   198,    26,    25,   718,\n",
      "          198,  6936,    25,   807,   198, 39471,    25,   362,   198,    13,\n",
      "           25,   642,   198,  2399,    25,   604,   198, 25771,    25,   838,\n",
      "          198,  3818,    25,   657,   198,    11,    25,   352,   198,   509,\n",
      "           25,   604,   198,  7878,    25,   767,   198,  1806,    25,   767,\n",
      "          198,    11,    25,   352,   198,  6149,    25,   604,   198,  6026,\n",
      "           25,   604,   198,   701,    25,   657,   198,  2704,    25,   807,\n",
      "          198, 11404,    25,   767,   198,   362,    25,   767,   198,   284,\n",
      "           25,   352,   198,  6129,    25,   513,   198,  6936,    25,   838,\n",
      "          198, 39471,    25,   657,   198, 10566,    25,   657,   198,    11,\n",
      "           25,   362,   198,  1262,    25,   604,   198, 26127,    25,   718,\n",
      "          198,   290,    25,   352,   198,   367,    25,   767,   198,   641,\n",
      "           25,   642,   198, 29232,    25,   352,   198,   367,    25,   718,\n",
      "          198,    82,    25,   513,   198, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch || :   2%|▏         | 2/100.0 [00:00<00:25,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13807, 18320,   265,  2522, 21897,   220,   357, 14508,  1900,   355,\n",
      "         2462, 18320,   265, 20106,  7833,   290,   319, 10724, 30248,   297,\n",
      "          283, 41927,   265, 20106,  7833,     8,   318,   257,  1006, 35587,\n",
      "        21998,  8771,   287,   543,   257, 30248,  8466,   286,   257, 17052,\n",
      "         1162, 39718,   318, 37100,  4126,  4291,   262, 32700,  4417,   286,\n",
      "          262,  5827,   338,  1162, 39718,    13,   317, 30248,   297,   283,\n",
      "         1221,   422,   257, 17052,  1162, 39718,   318,  4624,   625,   262,\n",
      "          390,    12,   538,   270, 35566,  1143,  2583,  1162, 39718,   290,\n",
      "          264,   315,  1522,   656,   257,  5597, 35096,   319,   262,  2583,\n",
      "         1162, 39718,    13,  1423,  3736,  2291,  3513,   286, 41927,   265,\n",
      "        36221,   385,    11,  1006, 35587,  8563,   588,   616, 24464,   290,\n",
      "         1029,  8718,  4164,  1773,   544,  1390, 49812, 21897,    11,   543,\n",
      "         2314,   307, 19267,   351,  5940,  5050,    13,   628,   198, 38143,\n",
      "         3736,   220,   198, 17227, 19481,   286,  2462, 18320,   265,  2522,\n",
      "        21897,  2291, 11038,  1281, 28609,  5874,  7628,    11,  5322,  1266,\n",
      "        19267,  5874,   936, 14834,    11, 20573, 21240, 35566, 22448,   290,\n",
      "        21388,  6468, 32441,  1042,    13,   198,   198, 18122,   220,   198,\n",
      "          818, 24977,    11, 36997,  2409,   430, 10819,  5495,  1006, 35587,\n",
      "         8771,   286, 14900,   286,   257, 26269,   291,  2261,  1626,   262,\n",
      "         1162,   710,   282,   336,   398,   282,  7679,    13,   554,  7169,\n",
      "          198,    31,  2235, 13702,  2235,    31,   277, 16993,    62, 17966,\n",
      "        19510, 41433,  4008, 31175, 16175,     4, 16175, 31175,   198, 18320,\n",
      "           25,   838,   198,   265,    25,   860,   198,  2522,    25,   767,\n",
      "          198, 21897,    25,   838,   198,   220,    25,   807,   198,   357,\n",
      "           25,   807,   198, 14508,    25,   604,   198,  1900,    25,   352,\n",
      "          198,   355,    25,   657,   198,  2462,    25,   767,   198, 18320,\n",
      "           25,   718,   198,   265,    25,   657,   198, 20106,    25,   807,\n",
      "          198,  7833,    25,   513,   198,   290,    25,   604,   198,   319,\n",
      "           25,   767,   198, 10724,    25,   838,   198, 30248,    25,  1511,\n",
      "          198,   297,    25,   657,   198,   283,    25,   352,   198, 41927,\n",
      "           25,   718,   198,   265,    25,   657,   198, 20106,    25,   362,\n",
      "          198,  7833,    25,   657,   198,     8,    25,   352,   198,   318,\n",
      "           25,   352,   198,   257,    25,   352,   198,  1006,    25,  1367,\n",
      "          198, 35587,    25,   657,   198, 21998,    25,   604,   198,  8771,\n",
      "           25,   657,   198,   287,    25,   362,   198,   543,    25,   657,\n",
      "          198,   257,    25,   352,   198, 30248,    25,   860,   198,  8466,\n",
      "           25,   604,   198,   286,    25,   513,   198,   257,    25,   362,\n",
      "          198, 17052,    25,   604,   198,  1162,    25,   807,   198, 39718,\n",
      "           25,   657,   198,   318,    25,   657,   198, 37100,    25,   642,\n",
      "          198,  4126,    25,   657,   198,  4291,    25,   362,   198,   262,\n",
      "           25,   352,   198, 32700,    25,   767,   198,  4417,    25,   362,\n",
      "          198,   286,    25,   657,   198,   262,    25,   657,   198,  5827,\n",
      "           25,   513,   198,   338,    25,   657,   198,  1162,    25,   362,\n",
      "          198, 39718,    25,   657,   198,    13,    25,   352,   198,   317,\n",
      "           25,   604,   198, 30248,    25,   604,   198,   297,    25,   352,\n",
      "          198,   283,    25,   657,   198,  1221,    25,   718,   198,   422,\n",
      "           25,   642,   198,   257,    25,   362,   198, 17052,    25,   657,\n",
      "          198,  1162,    25,   352,   198, 39718,    25,   657,   198,   318,\n",
      "           25,   657,   198,  4624,    25,   362,   198,   625,    25,   513,\n",
      "          198,   262,    25,   657,   198,   390,    25,   807,   198,    12,\n",
      "           25,   604,   198,   538,    25,   718,   198,   270,    25,   642,\n",
      "          198, 35566,    25,   352,   198,  1143,    25,   352,   198,  2583,\n",
      "           25,   807,   198,  1162,    25,   362,   198, 39718,    25,   657,\n",
      "          198,   290,    25,   352,   198,   264,    25,   807,   198,   315,\n",
      "           25,   657,   198,  1522,    25,   657,   198,   656,    25,   362,\n",
      "          198,   257,    25,   362,   198,  5597,    25,   860,   198, 35096,\n",
      "           25,   807,   198,   319,    25,   513,   198,   262,    25,   657,\n",
      "          198,  2583,    25,   642,   198,  1162,    25,   352,   198, 39718,\n",
      "           25,   657,   198,    13,    25,   352,   198,  1423,    25,  1105,\n",
      "          198,  3736,    25,   362,   198,  2291,    25,   513,   198,  3513,\n",
      "           25,   767,   198,   286,    25,   657,   198, 41927,    25,   642,\n",
      "          198,   265,    25,   657,   198, 36221,    25,   513,   198,   385,\n",
      "           25,   352,   198,    11,    25,   352,   198,  1006,    25,   642,\n",
      "          198, 35587,    25,   657,   198,  8563,    25,   604,   198,   588,\n",
      "           25,   807,   198,   616,    25,   604,   198, 24464,    25,   657,\n",
      "          198,   290,    25,   362,   198,  1029,    25,   718,   198,  8718,\n",
      "           25,   718,   198,  4164,    25,   718,   198,  1773,    25,   718,\n",
      "          198,   544,    25,   362,   198,  1390,    25,   807,   198, 49812,\n",
      "           25,  1105,   198, 21897,    25,   718,   198,    11,    25,   352,\n",
      "          198,   543,    25,   642,   198,  2314,    25,   767,   198,   307,\n",
      "           25,   657,   198, 19267,    25,   362,   198,   351,    25,   362,\n",
      "          198,  5940,    25,   807,   198,  5050,    25,   642,   198,    13,\n",
      "           25,   352,   198,   628,    25,   604,   198,   198,    25,   657,\n",
      "          198, 38143,    25,   767,   198,  3736,    25,   657,   198,   220,\n",
      "           25,   860,   198,   198,    25,   352,   198, 17227,    25,  2310,\n",
      "          198, 19481,    25,   352,   198,   286,    25,   352,   198,  2462,\n",
      "           25,   352,   198, 18320,    25,   657,   198,   265,    25,   657,\n",
      "          198,  2522,    25,   657,   198, 21897,    25,   657,   198,  2291,\n",
      "           25,   657,   198, 11038,    25,   767,   198,  1281,    25,   642,\n",
      "          198, 28609,    25,   718,   198,  5874,    25,   767,   198,  7628,\n",
      "           25,   604,   198,    11,    25,   352,   198,  5322,    25,   604,\n",
      "          198,  1266,    25,  1367,   198, 19267,    25,   860,   198,  5874,\n",
      "           25,   362,   198,   936,    25,   362,   198, 14834,    25,   657,\n",
      "          198,    11,    25,   352,   198, 20573,    25,   718,   198, 21240,\n",
      "           25,   807,   198, 35566,    25,   657,   198, 22448,    25,   807,\n",
      "          198,   290,    25,   362,   198, 21388,    25,   718,   198,  6468,\n",
      "           25,   767,   198, 32441,    25,   657,   198,  1042,    25,   657,\n",
      "          198,    13,    25,   352,   198,   198,    25,   362,   198,   198,\n",
      "           25,   657,   198, 18122,    25,   718,   198,   220,    25,   352,\n",
      "          198,   198,    25,   657,   198,   818,    25,  1467,   198, 24977,\n",
      "           25,   767,   198,    11,    25,   657,   198, 36997,    25,   838,\n",
      "          198,  2409,    25,   718,   198,   430,    25,   513,   198, 10819,\n",
      "           25,   838,   198,  5495,    25,   718,   198,  1006,    25,   860,\n",
      "          198, 35587,    25,   657,   198,  8771,    25,   718,   198,   286,\n",
      "           25,   604,   198, 14900,    25,   838,   198,   286,    25,   657,\n",
      "          198,   257,    25,   362,   198, 26269,    25,   767,   198,   291,\n",
      "           25,   513,   198,  2261,    25,   513,   198,  1626,    25,   642,\n",
      "          198,   262,    25,   352,   198,  1162,    25,   352,   198,   710,\n",
      "           25,   362,   198,   282,    25,   657,   198,   336,    25,   767,\n",
      "          198,   398,    25,   604,   198,   282,    25,   657,   198,  7679,\n",
      "           25,   642,   198,    13,    25,   362,   198,   554,    25,   513,\n",
      "          198,  7169,    25,   642,   198, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch || :   3%|▎         | 3/100.0 [00:00<00:25,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  379,  9666,   440,  4029,   324,   500,   287,  3944,  9526,   290,\n",
      "          366, 38432, 25370,    94,  1462, 17266, 32790,    83,   378,   267,\n",
      "          264,   260, 38325,    72,     1,  5855,  1639,   508,  4320,   286,\n",
      "        12157, 11074,   198,   198,   464,  7795,  5062, 40584,  8096,   262,\n",
      "         2647,   262,  4097,  6264,   329,   262, 10343, 32790,   272,   509,\n",
      "        10071, 46195,  1990,    72, 38325, 13766,   711,   286,   262,   976,\n",
      "         1438,    13,   383,  5062,  8096,   262,  7259,    11,   366,    43,\n",
      "           73,   549,   615,     1,  5855, 18565, 12340,  9593,  1085, 25355,\n",
      "          416,   262, 20495,  8674, 21798,   449,  1990,    83, 47297, 38325,\n",
      "           11,   290,   366,  5005,    89, 30530,   504,     1,  5855,  5005,\n",
      "        30530,   415, 12340,   290,   262,  1334,   286,   262,  2716,  2587,\n",
      "        19954,   286, 21543,  8339,    11,  1390,   257,  3002,  2196,   286,\n",
      "        32839,  7567,   626,   338, 10797,  2634,   305,    13,  5856,   262,\n",
      "          976,   614,    11,   262,  4097,  2630,  2647,   329,  1194,   711,\n",
      "           11,   311,  5362, 32790,    64,   509, 10071, 46195,  1990,    72,\n",
      "        38325,   338,   509,  1373,    73,  1990,    72, 38325,  2940,    78,\n",
      "           11,   287,   543,   262,  4097,  4120,   355,  2107, 23827,   290,\n",
      "         7205,  4159,  9176,    13,   383,  4097,   635,  4120,   319,   262,\n",
      "         1583,   129,   122,   470,    68,   474,  4449,     0,  1675,   299,\n",
      "        46313, 28428, 10185,   357,  3855,   705, 10161,     0,  1119,  4231,\n",
      "          198,    31,  2235, 13702,  2235,    31,   277, 16993,    62, 17966,\n",
      "        19510, 41433,  4008, 31175, 16175,     4, 16175, 31175,   198,  9666,\n",
      "           25,  1367,   198,   440,    25,   767,   198,  4029,    25,   860,\n",
      "          198,   324,    25,   807,   198,   500,    25,   807,   198,   287,\n",
      "           25,   604,   198,  3944,    25,   767,   198,  9526,    25,   657,\n",
      "          198,   290,    25,   513,   198,   366,    25,   767,   198, 38432,\n",
      "           25,   838,   198, 25370,    25,  1367,   198,    94,    25,   362,\n",
      "          198,  1462,    25,   767,   198, 17266,    25,   860,   198, 32790,\n",
      "           25,   362,   198,    83,    25,   362,   198,   378,    25,   767,\n",
      "          198,   267,    25,   642,   198,   264,    25,   642,   198,   260,\n",
      "           25,   807,   198, 38325,    25,   642,   198,    72,    25,   513,\n",
      "          198,     1,    25,   513,   198,  5855,    25,   362,   198,  1639,\n",
      "           25,   642,   198,   508,    25,   718,   198,  4320,    25,   767,\n",
      "          198,   286,    25,   352,   198, 12157,    25,   642,   198, 11074,\n",
      "           25,   362,   198,   198,    25,   352,   198,   198,    25,   657,\n",
      "          198,   464,    25,   362,   198,  7795,    25,   838,   198,  5062,\n",
      "           25,   718,   198, 40584,    25,  1105,   198,  8096,    25,   718,\n",
      "          198,   262,    25,   362,   198,  2647,    25,   642,   198,   262,\n",
      "           25,   767,   198,  4097,    25,   362,   198,  6264,    25,   362,\n",
      "          198,   329,    25,   362,   198,   262,    25,   352,   198, 10343,\n",
      "           25,   838,   198, 32790,    25,   352,   198,   272,    25,   604,\n",
      "          198,   509,    25,   718,   198, 10071,    25,   718,   198, 46195,\n",
      "           25,   352,   198,  1990,    25,   642,   198,    72,    25,   362,\n",
      "          198, 38325,    25,   352,   198, 13766,    25,   860,   198,   711,\n",
      "           25,   718,   198,   286,    25,   642,   198,   262,    25,   362,\n",
      "          198,   976,    25,   352,   198,  1438,    25,   657,   198,    13,\n",
      "           25,   352,   198,   383,    25,   362,   198,  5062,    25,   513,\n",
      "          198,  8096,    25,   513,   198,   262,    25,   362,   198,  7259,\n",
      "           25,   513,   198,    11,    25,   604,   198,   366,    25,   352,\n",
      "          198,    43,    25,   604,   198,    73,    25,   718,   198,   549,\n",
      "           25,   352,   198,   615,    25,   642,   198,     1,    25,   604,\n",
      "          198,  5855,    25,   352,   198, 18565,    25,   513,   198, 12340,\n",
      "           25,   362,   198,  9593,    25,   807,   198,  1085,    25,   767,\n",
      "          198, 25355,    25,   362,   198,   416,    25,   352,   198,   262,\n",
      "           25,   513,   198, 20495,    25,  1511,   198,  8674,    25,   604,\n",
      "          198, 21798,    25,   642,   198,   449,    25,   604,   198,  1990,\n",
      "           25,   604,   198,    83,    25,   642,   198, 47297,    25,   513,\n",
      "          198, 38325,    25,   657,   198,    11,    25,   352,   198,   290,\n",
      "           25,   352,   198,   366,    25,   657,   198,  5005,    25,   718,\n",
      "          198,    89,    25,   718,   198, 30530,    25,  1367,   198,   504,\n",
      "           25,   860,   198,     1,    25,   362,   198,  5855,    25,   657,\n",
      "          198,  5005,    25,   767,   198, 30530,    25,   604,   198,   415,\n",
      "           25,   513,   198, 12340,    25,   362,   198,   290,    25,   642,\n",
      "          198,   262,    25,   362,   198,  1334,    25,   718,   198,   286,\n",
      "           25,   657,   198,   262,    25,   657,   198,  2716,    25,   860,\n",
      "          198,  2587,    25,   362,   198, 19954,    25,   718,   198,   286,\n",
      "           25,   657,   198, 21543,    25,   718,   198,  8339,    25,   513,\n",
      "          198,    11,    25,   362,   198,  1390,    25,   362,   198,   257,\n",
      "           25,   513,   198,  3002,    25,   513,   198,  2196,    25,   513,\n",
      "          198,   286,    25,   657,   198, 32839,    25,  1367,   198,  7567,\n",
      "           25,   642,   198,   626,    25,   362,   198,   338,    25,   657,\n",
      "          198, 10797,    25,  1367,   198,  2634,    25,   642,   198,   305,\n",
      "           25,   642,   198,    13,    25,   513,   198,  5856,    25,   718,\n",
      "          198,   262,    25,   352,   198,   976,    25,   604,   198,   614,\n",
      "           25,   352,   198,    11,    25,   657,   198,   262,    25,   352,\n",
      "          198,  4097,    25,   352,   198,  2630,    25,   642,   198,  2647,\n",
      "           25,   642,   198,   329,    25,   657,   198,  1194,    25,   642,\n",
      "          198,   711,    25,   604,   198,    11,    25,   362,   198,   311,\n",
      "           25,   767,   198,  5362,    25,   807,   198, 32790,    25,   352,\n",
      "          198,    64,    25,   513,   198,   509,    25,   513,   198, 10071,\n",
      "           25,   362,   198, 46195,    25,   657,   198,  1990,    25,   657,\n",
      "          198,    72,    25,   657,   198, 38325,    25,   657,   198,   338,\n",
      "           25,   362,   198,   509,    25,   718,   198,  1373,    25,   767,\n",
      "          198,    73,    25,   513,   198,  1990,    25,   513,   198,    72,\n",
      "           25,   352,   198, 38325,    25,   657,   198,  2940,    25,   838,\n",
      "          198,    78,    25,   513,   198,    11,    25,   362,   198,   287,\n",
      "           25,   513,   198,   543,    25,   352,   198,   262,    25,   352,\n",
      "          198,  4097,    25,   362,   198,  4120,    25,   718,   198,   355,\n",
      "           25,   352,   198,  2107,    25,   718,   198, 23827,    25,   362,\n",
      "          198,   290,    25,   513,   198,  7205,    25,   860,   198,  4159,\n",
      "           25,   860,   198,  9176,    25,   352,   198,    13,    25,   657,\n",
      "          198,   383,    25,   362,   198,  4097,    25,   362,   198,   635,\n",
      "           25,   352,   198,  4120,    25,   604,   198,   319,    25,   352,\n",
      "          198,   262,    25,   352,   198,  1583,    25,   807,   198,   129,\n",
      "           25,   642,   198,   122,    25,   352,   198,   470,    25,  1367,\n",
      "          198,    68,    25,   513,   198,   474,    25,   718,   198,  4449,\n",
      "           25,   767,   198,     0,    25,   838,   198,  1675,    25,   860,\n",
      "          198,   299,    25,   838,   198, 46313,    25,   767,   198, 28428,\n",
      "           25,  1315,   198, 10185,    25,  1367,   198,   357,    25,   513,\n",
      "          198,  3855,    25,   767,   198,   705,    25,   767,   198, 10161,\n",
      "           25,   362,   198,     0,    25,   642,   198,  1119,    25,   718,\n",
      "          198,  4231,    25,   604,   198, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch || :   3%|▎         | 3/100.0 [00:01<00:33,  2.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/xent-futurarium/xent/trainer.py:175\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, saving_options, saving_info)\u001b[0m\n\u001b[1;32m    173\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    174\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM\u001b[38;5;241m.\u001b[39mmodel(input_ids\u001b[38;5;241m=\u001b[39mtokens)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m--> 175\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_batch_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    177\u001b[0m valloss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([valloss, loss\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)])\n",
      "File \u001b[0;32m~/xent-futurarium/xent/trainer.py:224\u001b[0m, in \u001b[0;36mTrainer.compute_batch_loss\u001b[0;34m(self, logits, tokens)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_batch_loss\u001b[39m(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    220\u001b[0m         logits, \u001b[38;5;66;03m# [B, T, V]\u001b[39;00m\n\u001b[1;32m    221\u001b[0m         tokens, \u001b[38;5;66;03m# [B, T]\u001b[39;00m\n\u001b[1;32m    222\u001b[0m         ):\n\u001b[1;32m    223\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: xidx, xlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_xstring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxreturn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m    226\u001b[0m         tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in the data, skipping... \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/xent-futurarium/xent/trainer.py:240\u001b[0m, in \u001b[0;36mTrainer.find_xstring\u001b[0;34m(self, tokens, string, return_len)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_xstring\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, string, return_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m#TODO this method exists both in Task() and Trainer() classes. Should make it unique. \u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Returns the index at which the xent function starts, needed for starting the loss computation \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     xdefseq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    241\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m xdefseq\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    242\u001b[0m     windows \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39munfold(dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39mseq_len, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/xent-futurarium/xent/base.py:88\u001b[0m, in \u001b[0;36mM.tokenize\u001b[0;34m(self, text, padding)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     77\u001b[0m         text: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     78\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_not_pad\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" The tokenize method returns a dictionary with following keys: \u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        input_ids:      Tensor with token keys\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m        attention_mask: Tensor with the attention masking \"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx_window\u001b[49m\n\u001b[0;32m---> 88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/xent-futurarium/menv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:816\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/xent-futurarium/menv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:816\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(output) > 0:\n",
    "#     print(\"YES\")\n",
    "#     print(tensor.shape)\n",
    "#     print(lenprompt)\n",
    "#     print(tensor[0, lenprompt:].shape)\n",
    "# else:\n",
    "#     print(\"NOPE\")\n",
    "#     print(tensor.shape)\n",
    "#     print(lenprompt)\n",
    "#     print(tensor[0, lenprompt:].shape)\n",
    "\n",
    "# print(prompt)\n",
    "# print(output[:40])\n",
    "# print(\"----------- TRUE ------------\")\n",
    "# print(true[:40])\n",
    "# print(tensor)\n",
    "# print(lenprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oner on Psych and another on Supernatural, depicting a malevolent angel called Zachariah. Fuller later reprised the role for Supernatural's 300th episode \"Lebanon.\" He also acted as a Senior Police Officer in the 2000 comedy Scary Movie, who is the commanding officer of Doofy.\n",
      "\n",
      "In 1986, he played \"Frank\" in the US premiere of Steven Berkoff's play Kvetch in West Los Angeles. The following year he reprised the role Off-Broadway at the Westside Arts Theater.\n",
      "\n",
      "From 2010 to 2011, Fuller appeared in the ABC Network series Better with You: the show was canceled after a single season.\n",
      "\n",
      "Fuller portrayed director of the C.I.A., Grayden Osborne, in the ABC drama Scandal. He also appears in FOX's Us & Them with Jane Kaczmarek and Jason Ritter.\n",
      "\n",
      "Personal life\n",
      "\n",
      "Fuller was born in San Francisco, California but raised\n",
      "@##$$##@ fwd_closure((integer))>:ç%ç>: the:: 5\n",
      " Psych: 13\n",
      " and: 5\n",
      " another: 9\n",
      " on: 6\n",
      " Super: 9\n",
      "natural: 1\n",
      ",: 2\n",
      " depicting: 8\n",
      " a: 1\n",
      " male: 7\n",
      "volent: 2\n",
      " angel: 7\n",
      " called: 4\n",
      " Zach: 8\n",
      "ar: 3\n",
      "iah: 0\n",
      ".: 2\n",
      " Fuller: 11\n",
      " later: 7\n",
      " repr: 7\n",
      "ised: 0\n",
      " the: 1\n",
      " role: 0\n",
      " for: 2\n",
      " Super: 5\n",
      "natural: 0\n",
      "'s: 3\n",
      " 300: 9\n",
      "th: 0\n",
      " episode: 5\n",
      " \": 3\n",
      "L: 5\n",
      "eb: 7\n",
      "anon: 2\n",
      ".\": 3\n",
      " He: 4\n",
      " also: 2\n",
      " acted: 5\n",
      " as: 2\n",
      " a: 2\n",
      " Senior: 6\n",
      " Police: 6\n",
      " Officer: 0\n",
      " in: 2\n",
      " the: 1\n",
      " 2000: 6\n",
      " comedy: 6\n",
      " Sc: 8\n",
      "ary: 4\n",
      " Movie: 1\n",
      ",: 2\n",
      " who: 6\n",
      " is: 4\n",
      " the: 4\n",
      " commanding: 4\n",
      " officer: 0\n",
      " of: 0\n",
      " Do: 11\n",
      "of: 5\n",
      "y: 3\n",
      ".: 2\n",
      "\n",
      ": 1\n",
      "\n",
      ": 0\n",
      "In: 3\n",
      " 1986: 6\n",
      ",: 0\n",
      " he: 2\n",
      " played: 2\n",
      " \": 5\n",
      "Frank: 7\n",
      "\": 2\n",
      " in: 1\n",
      " the: 1\n",
      " US: 7\n",
      " premiere: 7\n",
      " of: 0\n",
      " Steven: 8\n",
      " Berk: 10\n",
      "off: 7\n",
      "'s: 0\n",
      " play: 5\n",
      " K: 8\n",
      "vet: 10\n",
      "ch: 3\n",
      " in: 3\n",
      " West: 8\n",
      " Los: 1\n",
      " Angeles: 0\n",
      ".: 0\n",
      " The: 3\n",
      " following: 3\n",
      " year: 0\n",
      " he: 2\n",
      " repr: 2\n",
      "ised: 0\n",
      " the: 1\n",
      " role: 0\n",
      " Off: 16\n",
      "-: 2\n",
      "Broad: 0\n",
      "way: 0\n",
      " at: 4\n",
      " the: 1\n",
      " West: 6\n",
      "side: 6\n",
      " Arts: 7\n",
      " Theater: 4\n",
      ".: 2\n",
      "\n",
      ": 1\n",
      "\n",
      ": 0\n",
      "From: 6\n",
      " 2010: 6\n",
      " to: 1\n",
      " 2011: 2\n",
      ",: 0\n",
      " Fuller: 1\n",
      " appeared: 3\n",
      " in: 1\n",
      " the: 2\n",
      " ABC: 4\n",
      " Network: 7\n",
      " series: 3\n",
      " Better: 8\n",
      " with: 8\n",
      " You: 5\n",
      ":: 5\n",
      " the: 5\n",
      " show: 5\n",
      " was: 5\n",
      " canceled: 7\n",
      " after: 1\n",
      " a: 2\n",
      " single: 5\n",
      " season: 1\n",
      ".: 1\n",
      "\n",
      ": 1\n",
      "\n",
      ": 0\n",
      "Full: 3\n",
      "er: 0\n",
      " portrayed: 7\n",
      " director: 9\n",
      " of: 4\n",
      " the: 1\n",
      " C: 7\n",
      ".: 2\n",
      "I: 2\n",
      ".: 0\n",
      "A: 2\n",
      ".,: 2\n",
      " Gray: 9\n",
      "den: 8\n",
      " Osborne: 9\n",
      ",: 1\n",
      " in: 1\n",
      " the: 1\n",
      " ABC: 4\n",
      " drama: 3\n",
      " Sc: 6\n",
      "andal: 1\n",
      ".: 1\n",
      " He: 2\n",
      " also: 1\n",
      " appears: 5\n",
      " in: 0\n",
      " FOX: 7\n",
      "'s: 0\n",
      " Us: 9\n",
      " &: 1\n",
      " Them: 0\n",
      " with: 2\n",
      " Jane: 6\n",
      " K: 3\n",
      "ac: 0\n",
      "z: 0\n",
      "mare: 0\n",
      "k: 0\n",
      " and: 2\n",
      " Jason: 5\n",
      " R: 5\n",
      "itter: 0\n",
      ".: 0\n",
      "\n",
      ": 1\n",
      "\n",
      ": 0\n",
      "Personal: 4\n",
      " life: 2\n",
      "\n",
      ": 1\n",
      "\n",
      ": 0\n",
      "Full: 0\n",
      "er: 0\n",
      " was: 2\n",
      " born: 0\n",
      " in: 1\n",
      " San: 4\n",
      " Francisco: 1\n",
      ",: 1\n",
      " California: 0\n",
      " but: 5\n",
      " raised: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.detokenize(tensor, \"single\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [\" 0\",\" 1\",\" 2\",\" 3\",\" 4\",\" 5\",\" 6\",\" 7\",\" 8\",\" 9\",\" 10\",\" 11\",\" 12\",\" 13\",\" 14\",\" 15\",\" 16\",\" 17\",\" 18\",\" 19\",\" 20\"]\n",
    "numtoks = torch.tensor([model.tokenize(num).input_ids for num in numbers]).to(device)\n",
    "logit_vector = torch.full((model.model.config.vocab_size,), float(-100), device=device)\n",
    "logit_vector[numtoks.flatten()] = torch.ones(numtoks.flatten().shape[0], device=device) # set to ones instead of random values\n",
    "\n",
    "# logit_vector = torch.ones(model.model.config.vocab_size, device=device)\n",
    "# logit_vector[numtoks.flatten()] = torch.rand(numtoks.flatten().shape[0], device=device) * 0.2 + 0.9 # example of a random logits vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:26<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7929180550575254\n",
      "3.281314630508423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "checker_model.model.eval()\n",
    "model_loss_on_nums = []\n",
    "random_loss_on_nums = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for n in tqdm(range(100)):\n",
    "        \n",
    "        synth = task.generate(get_test_sample, space=\"tokens\")\n",
    "        cut, xlen = task.find_xstring(synth, X.xreturn, return_len=True)\n",
    "        CUT = cut + xlen + 1 # +1 is for the newline \\n\n",
    "        genshift = 1\n",
    "        \n",
    "        logits = checker_model.model(synth).logits\n",
    "        # random_logits = torch.ones_like(logits)\n",
    "        cut_logits = logits[0, CUT-genshift:-1]\n",
    "        random_logits = cut_logits.clone()\n",
    "        # random_logits = random_logits[0, CUT-genshift:-1]\n",
    "\n",
    "        # print(logits.shape)\n",
    "        # print(synth.shape)\n",
    "\n",
    "        for pos in torch.arange(2, cut_logits.shape[0], 4):\n",
    "            new_random_logits = logit_vector\n",
    "            new_random_logits[numtoks.flatten()] = torch.rand(numtoks.flatten().shape[0], device=device) * 0.2 + 0.9\n",
    "            random_logits[pos] = new_random_logits\n",
    "\n",
    "        model_loss = F.cross_entropy(cut_logits, synth[0, CUT-genshift+1:], reduction=\"none\")\n",
    "        random_loss = F.cross_entropy(random_logits, synth[0, CUT-genshift+1:], reduction=\"none\")\n",
    " \n",
    "        model_probs = F.softmax(cut_logits, dim=-1)\n",
    "        highest_prob_tokens = torch.argmax(model_probs, dim=-1)\n",
    "        model_values, model_indices = torch.topk(cut_logits, k=5)\n",
    "\n",
    "        random_probs = F.softmax(random_logits, dim=-1)\n",
    "        random_values, random_indices = torch.topk(random_logits, k=5)\n",
    "        \n",
    "        model_tot_loss = model_loss[2::4].mean().item()\n",
    "        model_loss_on_nums.append(model_tot_loss)\n",
    "\n",
    "        random_tot_loss = random_loss[2::4].mean().item()\n",
    "        random_loss_on_nums.append(random_tot_loss)\n",
    "\n",
    "print(sum(model_loss_on_nums)/len(model_loss_on_nums))\n",
    "print(sum(random_loss_on_nums)/len(random_loss_on_nums))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
