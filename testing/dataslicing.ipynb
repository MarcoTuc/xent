{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xent import M, device\n",
    "from xent.datasets import Wikipedia\n",
    "from xent.tasks import Closure\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M(\"gpt2\", \"M0\")\n",
    "task = Closure(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22eefb668a4346bd99a29d79e682c31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\n",
    "    \"wikipedia\", \n",
    "    \"20220301.en\", \n",
    "    trust_remote_code=True,\n",
    "    )[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = load_dataset(\n",
    "    \"wikipedia\", \n",
    "    \"20220301.en\", \n",
    "    trust_remote_code=True,\n",
    "    split=\"train[:50%]\"\n",
    "    )\n",
    "\n",
    "data_2 = load_dataset(\n",
    "    \"wikipedia\", \n",
    "    \"20220301.en\", \n",
    "    trust_remote_code=True,\n",
    "    split=\"train[50%:]\"\n",
    "    )\n",
    "\n",
    "data_slices = load_dataset(\n",
    "    \"wikipedia\", \n",
    "    \"20220301.en\", \n",
    "    trust_remote_code=True,\n",
    "    split=[f\"train[{k}%:{k+10}%]\" for k in range(0, 100, 10)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data.select(range(0,len(data)//2))\n",
    "data_2 = data.select(range(len(data)//2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shards = 7\n",
    "shards = []\n",
    "for s in range(n_shards):\n",
    "    shards.append(data_1.shard(num_shards=n_shards, index=s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice shape: torch.Size([1, 202])\n",
      "task shape: torch.Size([1, 19])\n",
      "slice again: torch.Size([1, 202])\n",
      "task again: torch.Size([1, 19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     get_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: data[n][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_parallel_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/xent-futurarium/xent/tasks.py:96\u001b[0m, in \u001b[0;36mClosure.generate_parallel_parallel\u001b[0;34m(self, get_sample)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;129;01min\u001b[39;00m toks:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     97\u001b[0m     xent \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mslice\u001b[39m[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# generate forward closure tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/xent-futurarium/menv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/xent-futurarium/menv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/xent-futurarium/menv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1316\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1316\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/xent-futurarium/menv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/xent-futurarium/menv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/xent-futurarium/menv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1003\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn_if_padding_and_no_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m   1005\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/xent-futurarium/menv/lib/python3.10/site-packages/transformers/modeling_utils.py:4755\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   4752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   4754\u001b[0m \u001b[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[0;32m-> 4755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m   4756\u001b[0m     warn_string \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4757\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4758\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4759\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4760\u001b[0m     )\n\u001b[1;32m   4762\u001b[0m     \u001b[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[1;32m   4763\u001b[0m     \u001b[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "generated = torch.LongTensor([]).to(device)\n",
    "for n in range(1):\n",
    "    get_data = lambda: data[n][\"text\"]\n",
    "    output = task.generate_parallel_parallel(get_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy. Anarchism calls for the abolition of the state, which it holds to be unnecessary, undesirable, and harmful. As a historically left-wing movement, placed on the farthest left of the political spectrum, it is usually described alongside communalism and libertarian Marxism as the libertarian wing (libertarian socialism) of the socialist movement, and has a strong historical association with anti-capitalism and socialism.\n",
      "\n",
      "Humans lived in societies without formal hierarchies long before the establishment of formal states, realms, or empires. With the rise of organised hierarchical bodies, scepticism toward authority also rose. Although traces of anarchist thought are found throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. Various anarchist\n",
      "@##$$##@ fwd_closure((integer))>:ç%ç>:\n",
      "arch: 10\n",
      "ism: 2\n",
      " is: 2\n",
      " a: 2\n",
      " political: 3\n",
      " philosophy: 2\n",
      " and: 4\n",
      " movement: 5\n",
      " that: 1\n",
      " is: 3\n",
      " scept: 12\n",
      "ical: 0\n",
      " of: 0\n",
      " authority: 5\n",
      " and: 1\n",
      " rejects: 4\n",
      " all: 3\n",
      " involuntary: 13\n",
      ",: 4\n",
      " coercive: 3\n",
      " forms: 3\n",
      " of: 0\n",
      " hierarchy: 5\n",
      ".: 1\n",
      " Anarch: 2\n",
      "ism: 1\n",
      " calls: 5\n",
      " for: 0\n",
      " the: 2\n",
      " abolition: 1\n",
      " of: 0\n",
      " the: 2\n",
      " state: 2\n",
      ",: 1\n",
      " which: 3\n",
      " it: 3\n",
      " holds: 6\n",
      " to: 1\n",
      " be: 0\n",
      " unnecessary: 5\n",
      ",: 2\n",
      " undesirable: 7\n",
      ",: 1\n",
      " and: 1\n",
      " harmful: 3\n",
      ".: 1\n",
      " As: 4\n",
      " a: 2\n",
      " historically: 10\n",
      " left: 7\n",
      "-: 1\n",
      "wing: 0\n",
      " movement: 1\n",
      ",: 0\n",
      " placed: 14\n",
      " on: 2\n",
      " the: 1\n",
      " fart: 11\n",
      "hest: 0\n",
      " left: 1\n",
      " of: 1\n",
      " the: 1\n",
      " political: 2\n",
      " spectrum: 0\n",
      ",: 0\n",
      " it: 2\n",
      " is: 2\n",
      " usually: 7\n",
      " described: 4\n",
      " alongside: 9\n",
      " communal: 10\n",
      "ism: 1\n",
      " and: 1\n",
      " libertarian: 4\n",
      " Marxism: 8\n",
      " as: 2\n",
      " the: 2\n",
      " libertarian: 6\n",
      " wing: 5\n",
      " (: 6\n",
      "liber: 6\n",
      "tarian: 0\n",
      " socialism: 3\n",
      "): 2\n",
      " of: 1\n",
      " the: 1\n",
      " socialist: 5\n",
      " movement: 1\n",
      ",: 2\n",
      " and: 2\n",
      " has: 4\n",
      " a: 2\n",
      " strong: 2\n",
      " historical: 5\n",
      " association: 6\n",
      " with: 0\n",
      " anti: 5\n",
      "-: 0\n",
      "capital: 3\n",
      "ism: 0\n",
      " and: 1\n",
      " socialism: 4\n",
      ".: 0\n",
      "\n",
      ": 1\n",
      "\n",
      ": 0\n",
      "Hum: 10\n",
      "ans: 0\n",
      " lived: 8\n",
      " in: 1\n",
      " societies: 3\n",
      " without: 4\n",
      " formal: 4\n",
      " hierarch: 3\n",
      "ies: 0\n",
      " long: 8\n",
      " before: 0\n",
      " the: 1\n",
      " establishment: 6\n",
      " of: 0\n",
      " formal: 3\n",
      " states: 5\n",
      ",: 2\n",
      " realms: 14\n",
      ",: 2\n",
      " or: 2\n",
      " empires: 5\n",
      ".: 0\n",
      " With: 6\n",
      " the: 1\n",
      " rise: 2\n",
      " of: 0\n",
      " organised: 7\n",
      " hierarchical: 8\n",
      " bodies: 6\n",
      ",: 1\n",
      " scept: 13\n",
      "icism: 1\n",
      " toward: 6\n",
      " authority: 1\n",
      " also: 6\n",
      " rose: 4\n",
      ".: 1\n",
      " Although: 5\n",
      " traces: 11\n",
      " of: 0\n",
      " anarchist: 5\n",
      " thought: 1\n",
      " are: 2\n",
      " found: 2\n",
      " throughout: 2\n",
      " history: 2\n",
      ",: 0\n",
      " modern: 6\n",
      " anarchism: 2\n",
      " emerged: 6\n",
      " from: 1\n",
      " the: 1\n",
      " Enlightenment: 4\n",
      ".: 3\n",
      " During: 6\n",
      " the: 0\n",
      " latter: 6\n",
      " half: 1\n",
      " of: 0\n",
      " the: 0\n",
      " 19: 1\n",
      "th: 0\n",
      " and: 3\n",
      " the: 4\n",
      " first: 4\n",
      " decades: 4\n",
      " of: 0\n",
      " the: 0\n",
      " 20: 0\n",
      "th: 0\n",
      " century: 0\n",
      ",: 0\n",
      " the: 2\n",
      " anarchist: 3\n",
      " movement: 0\n",
      " flourished: 5\n",
      " in: 2\n",
      " most: 5\n",
      " parts: 3\n",
      " of: 0\n",
      " the: 0\n",
      " world: 0\n",
      " and: 3\n",
      " had: 4\n",
      " a: 1\n",
      " significant: 3\n",
      " role: 3\n",
      " in: 0\n",
      " workers: 11\n",
      "': 0\n",
      " struggles: 2\n",
      " for: 3\n",
      " emancipation: 4\n",
      ".: 1\n",
      " Various: 7\n",
      " anarchist: 3\n",
      "\n",
      "Anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy. Anarchism calls for the abolition of the state, which it holds to be unnecessary, undesirable, and harmful. As a historically left-wing movement, placed on the farthest left of the political spectrum, it is usually described alongside communalism and libertarian Marxism as the libertarian wing (libertarian socialism) of the socialist movement, and has a strong historical association with anti-capitalism and socialism.\n",
      "\n",
      "Humans lived in societies without formal hierarchies long before the establishment of formal states, realms, or empires. With the rise of organised hierarchical bodies, scepticism toward authority also rose. Although traces of anarchist thought are found throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. Various anarchist\n",
      "@##$$##@ bwd_closure((integer))>:ç%ç>:\n",
      " anarchist: 3\n",
      " Various: 7\n",
      ".: 1\n",
      " emancipation: 4\n",
      " for: 3\n",
      " struggles: 2\n",
      "': 0\n",
      " workers: 11\n",
      " in: 0\n",
      " role: 3\n",
      " significant: 3\n",
      " a: 1\n",
      " had: 4\n",
      " and: 3\n",
      " world: 0\n",
      " the: 0\n",
      " of: 0\n",
      " parts: 3\n",
      " most: 5\n",
      " in: 2\n",
      " flourished: 5\n",
      " movement: 0\n",
      " anarchist: 3\n",
      " the: 2\n",
      ",: 0\n",
      " century: 0\n",
      "th: 0\n",
      " 20: 0\n",
      " the: 0\n",
      " of: 0\n",
      " decades: 4\n",
      " first: 4\n",
      " the: 4\n",
      " and: 3\n",
      "th: 0\n",
      " 19: 1\n",
      " the: 0\n",
      " of: 0\n",
      " half: 1\n",
      " latter: 6\n",
      " the: 0\n",
      " During: 6\n",
      ".: 3\n",
      " Enlightenment: 4\n",
      " the: 1\n",
      " from: 1\n",
      " emerged: 6\n",
      " anarchism: 2\n",
      " modern: 6\n",
      ",: 0\n",
      " history: 2\n",
      " throughout: 2\n",
      " found: 2\n",
      " are: 2\n",
      " thought: 1\n",
      " anarchist: 5\n",
      " of: 0\n",
      " traces: 11\n",
      " Although: 5\n",
      ".: 1\n",
      " rose: 4\n",
      " also: 6\n",
      " authority: 1\n",
      " toward: 6\n",
      "icism: 1\n",
      " scept: 13\n",
      ",: 1\n",
      " bodies: 6\n",
      " hierarchical: 8\n",
      " organised: 7\n",
      " of: 0\n",
      " rise: 2\n",
      " the: 1\n",
      " With: 6\n",
      ".: 0\n",
      " empires: 5\n",
      " or: 2\n",
      ",: 2\n",
      " realms: 14\n",
      ",: 2\n",
      " states: 5\n",
      " formal: 3\n",
      " of: 0\n",
      " establishment: 6\n",
      " the: 1\n",
      " before: 0\n",
      " long: 8\n",
      "ies: 0\n",
      " hierarch: 3\n",
      " formal: 4\n",
      " without: 4\n",
      " societies: 3\n",
      " in: 1\n",
      " lived: 8\n",
      "ans: 0\n",
      "Hum: 10\n",
      "\n",
      ": 0\n",
      "\n",
      ": 1\n",
      ".: 0\n",
      " socialism: 4\n",
      " and: 1\n",
      "ism: 0\n",
      "capital: 3\n",
      "-: 0\n",
      " anti: 5\n",
      " with: 0\n",
      " association: 6\n",
      " historical: 5\n",
      " strong: 2\n",
      " a: 2\n",
      " has: 4\n",
      " and: 2\n",
      ",: 2\n",
      " movement: 1\n",
      " socialist: 5\n",
      " the: 1\n",
      " of: 1\n",
      "): 2\n",
      " socialism: 3\n",
      "tarian: 0\n",
      "liber: 6\n",
      " (: 6\n",
      " wing: 5\n",
      " libertarian: 6\n",
      " the: 2\n",
      " as: 2\n",
      " Marxism: 8\n",
      " libertarian: 4\n",
      " and: 1\n",
      "ism: 1\n",
      " communal: 10\n",
      " alongside: 9\n",
      " described: 4\n",
      " usually: 7\n",
      " is: 2\n",
      " it: 2\n",
      ",: 0\n",
      " spectrum: 0\n",
      " political: 2\n",
      " the: 1\n",
      " of: 1\n",
      " left: 1\n",
      "hest: 0\n",
      " fart: 11\n",
      " the: 1\n",
      " on: 2\n",
      " placed: 14\n",
      ",: 0\n",
      " movement: 1\n",
      "wing: 0\n",
      "-: 1\n",
      " left: 7\n",
      " historically: 10\n",
      " a: 2\n",
      " As: 4\n",
      ".: 1\n",
      " harmful: 3\n",
      " and: 1\n",
      ",: 1\n",
      " undesirable: 7\n",
      ",: 2\n",
      " unnecessary: 5\n",
      " be: 0\n",
      " to: 1\n",
      " holds: 6\n",
      " it: 3\n",
      " which: 3\n",
      ",: 1\n",
      " state: 2\n",
      " the: 2\n",
      " of: 0\n",
      " abolition: 1\n",
      " the: 2\n",
      " for: 0\n",
      " calls: 5\n",
      "ism: 1\n",
      " Anarch: 2\n",
      ".: 1\n",
      " hierarchy: 5\n",
      " of: 0\n",
      " forms: 3\n",
      " coercive: 3\n",
      ",: 4\n",
      " involuntary: 13\n",
      " all: 3\n",
      " rejects: 4\n",
      " and: 1\n",
      " authority: 5\n",
      " of: 0\n",
      "ical: 0\n",
      " scept: 12\n",
      " is: 3\n",
      " that: 1\n",
      " movement: 5\n",
      " and: 4\n",
      " philosophy: 2\n",
      " political: 3\n",
      " a: 2\n",
      " is: 2\n",
      "ism: 2\n",
      "arch: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.detokenize(output[0][0]))\n",
    "print(model.detokenize(output[4][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.ones(10)\n",
    "t = t.unfold(0,3,3)\n",
    "\n",
    "for row in t:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_corpus_method = lambda: data.database[91][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Closure.generate() got an unexpected keyword argument 'inverse_order'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_corpus_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/xent-futurarium/xent/base.py:217\u001b[0m, in \u001b[0;36mTask.synthesize\u001b[0;34m(self, get_sample, n_samples, out_type, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    216\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_generator(get_sample\u001b[38;5;241m=\u001b[39mget_sample, out_type\u001b[38;5;241m=\u001b[39mout_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m generator(n_samples):\n\u001b[1;32m    218\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(sample)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;66;03m# returns List[str]\u001b[39;00m\n",
      "File \u001b[0;32m~/xent-futurarium/xent/base.py:196\u001b[0m, in \u001b[0;36mTask.dataset_generator.<locals>.iterator\u001b[0;34m(n_samples)\u001b[0m\n\u001b[1;32m    194\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m<\u001b[39m n_samples:\n\u001b[0;32m--> 196\u001b[0m     tok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# generate should always move in tokens space\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tok\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM\u001b[38;5;241m.\u001b[39mctx_window:\n\u001b[1;32m    198\u001b[0m         n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Closure.generate() got an unexpected keyword argument 'inverse_order'"
     ]
    }
   ],
   "source": [
    "new_data = task.synthesize(get_corpus_method, 1, out_type=\"tokens\", inverse_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 30, 30, 30, 30, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 153\n",
    "chunks = 30\n",
    "\n",
    "def schedule(total, chunks):\n",
    "    if total%chunks != 0:\n",
    "        return [*[chunks]*(total//chunks), total%chunks]\n",
    "    else: \n",
    "        return [chunks]*(total//chunks)\n",
    "\n",
    "schedule(total, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers = 3\n",
    "\n",
    "def distribute_workers(total, chunks, workers):\n",
    "    splits = schedule(total, workers)\n",
    "    return splits\n",
    "    schedules = [schedule(s, chunks) for s in splits]\n",
    "    return schedules\n",
    "\n",
    "total = 100\n",
    "workers = 3\n",
    "chunks = 100\n",
    "\n",
    "per_worker = total // workers\n",
    "per_worker_schedule = [[*[chunks]*(per_worker//chunks), total%chunks] for _ in range(workers)]\n",
    "\n",
    "import torch\n",
    "torch.tensor(per_worker_schedule).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2152890, 2152890, 2152890]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_into_n(total, n):\n",
    "    base = total // n\n",
    "    remainder = total % n\n",
    "    result = [base] * n\n",
    "    if remainder:\n",
    "        result[-1] += remainder\n",
    "    return result\n",
    "\n",
    "split_into_n(6458670, 3)  # Returns [33, 33, 34]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
